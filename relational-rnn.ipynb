{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational RNNs by Adam Santoro et al. in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance as spdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_size: The size of the input features\n",
    "        mem_slots: The total number of memory slots to use.\n",
    "        head_size: The size of an attention head.\n",
    "        num_heads: The number of attention heads to use. Defaults to 1.\n",
    "        num_blocks: Number of times to compute attention per time step. Defaults to 1.\n",
    "        forget_bias: \n",
    "        input_bias:\n",
    "        gate_style:\n",
    "        attention_mlp_layers:\n",
    "        key_size:\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size,\n",
    "                 mem_slots, head_size, num_heads=1, num_blocks=1,\n",
    "                 forget_bias=1.0, input_bias=0.0, gate_style='unit',\n",
    "                 attention_mlp_layers=2, key_size=None):\n",
    "        super(RelationalMemory, self).__init__()\n",
    "        \n",
    "        self._mem_slots = mem_slots\n",
    "        self._head_size = head_size\n",
    "        self._num_heads = num_heads\n",
    "        self._mem_size = self._head_size * self._num_heads\n",
    "\n",
    "        if num_blocks < 1:\n",
    "            raise ValueError('num_blocks must be >= 1. Got: {}.'.format(num_blocks))\n",
    "        self._num_blocks = num_blocks\n",
    "\n",
    "        self._forget_bias = forget_bias\n",
    "        self._input_bias = input_bias\n",
    "\n",
    "        if gate_style not in ['unit', 'memory', None]:\n",
    "            raise ValueError(\n",
    "                'gate_style must be one of [\\'unit\\', \\'memory\\', None]. Got: '\n",
    "                '{}.'.format(gate_style))\n",
    "        self._gate_style = gate_style\n",
    "\n",
    "        if attention_mlp_layers < 1:\n",
    "            raise ValueError('attention_mlp_layers must be >= 1. Got: {}.'.format(\n",
    "                attention_mlp_layers))\n",
    "        self._attention_mlp_layers = attention_mlp_layers\n",
    "\n",
    "        self._key_size = key_size if key_size else self._head_size\n",
    "\n",
    "        self._linear = nn.Linear(in_features=input_size,\n",
    "                                 out_features=self._mem_size)\n",
    "        \n",
    "        qkv_size = 2 * self._key_size + self._head_size\n",
    "        total_size = qkv_size * self._num_heads\n",
    "        self._attention_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                           out_features=total_size)\n",
    "        self._attention_layer_norm = nn.LayerNorm(total_size)\n",
    "        \n",
    "        attention_mlp_modules = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=self._mem_size,\n",
    "                          out_features=self._mem_size),\n",
    "                nn.ReLU())] * (self._attention_mlp_layers - 1) +\n",
    "            [nn.Linear(in_features=self._mem_size,\n",
    "                       out_features=self._mem_size)]\n",
    "        )\n",
    "        self._attention_mlp = nn.Sequential(*attention_mlp_modules)\n",
    "        \n",
    "        self._attend_layer_norm_1 = nn.LayerNorm(self._mem_size)\n",
    "        self._attend_layer_norm_2 = nn.LayerNorm(self._mem_size)\n",
    "        \n",
    "        num_gates = 2 * self._calculate_gate_size()\n",
    "        self._gate_inputs_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                             out_features=num_gates)\n",
    "        \n",
    "        self._gate_hidden_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                             out_features=num_gates)\n",
    "        \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\"Creates the initial memory.\n",
    "\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size \n",
    "        (batch_size, self._mem_slots, self._mem_size).\n",
    "\n",
    "        Returns:\n",
    "            init_state: A truncated or padded matrix of size\n",
    "            (batch_size, self._mem_slots, self._mem_size).\n",
    "        \"\"\"    \n",
    "        init_state = torch.eye(n=self._mem_slots).repeat(batch_size, 1, 1)\n",
    "        \n",
    "        if self._mem_size > self._mem_slots:\n",
    "            # Pad the matrix with zeros.\n",
    "            difference = self._mem_size - self._mem_slots\n",
    "            pad = torch.zeros((batch_size, self._mem_slots, difference))\n",
    "            init_state = torch.cat([init_state, pad], dim=-1)\n",
    "        elif self._mem_size < self._mem_slots:\n",
    "            # Truncation. Take the first `self._mem_size` components.\n",
    "            init_state = init_state[:, :, :self._mem_size]\n",
    "        \n",
    "        return init_state.detach()\n",
    "\n",
    "    def _multihead_attention(self, memory): # memory: [B, MEM_SLOT, MEM_SIZE]\n",
    "        # F = total_size\n",
    "        # mem_slots = MEM_SLOT = N\n",
    "        mem_slots = memory.size(1)\n",
    "        \n",
    "        # [B, MEM_SLOT, MEM_SIZE] -> [B*MEM_SLOT, MEM_SIZE] -> Linear -> [B*MEM_SLOT, F]\n",
    "        qkv = self._attention_linear(memory.view(-1, memory.size(2)))\n",
    "        \n",
    "        # [B*MEM_SLOT, F] -> Layer Norm -> [B*MEM_SLOT, F] -> [B, MEM_SLOT, F]\n",
    "        qkv = self._attention_layer_norm(qkv).view(memory.size(0), mem_slots, -1)\n",
    "        \n",
    "        # H = num_heads\n",
    "        qkv_size = 2 * self._key_size + self._head_size\n",
    "        \n",
    "        # [B, N, F] -> [B, N, H, F/H]\n",
    "        qkv_reshape = qkv.view(-1, mem_slots, self._num_heads, qkv_size)\n",
    "        \n",
    "        # [B, N, H, F/H] -> [B, H, N, F/H]\n",
    "        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n",
    "        # split q, k, v\n",
    "        q, k, v = torch.split(qkv_transpose, [self._key_size, self._key_size, self._head_size], dim=-1)\n",
    "        \n",
    "        q *= qkv_size ** -0.5\n",
    "        dot_product = torch.matmul(q, torch.transpose(k, 2, 3)) # [B, H, N, N]\n",
    "        weights = F.softmax(dot_product, dim=-1)\n",
    "        \n",
    "        #[B, H, N, V]\n",
    "        output = torch.matmul(weights, v)\n",
    "        \n",
    "        # [B, H, N, V] -> [B, N, H, V]\n",
    "        output_transpose = output.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # [B, N, H, V] -> [B, N, H * V]\n",
    "        new_memory = output_transpose.contiguous().view(-1, output_transpose.size(1), \n",
    "                                                        output_transpose.size(2)*output_transpose.size(3))\n",
    "        \n",
    "        return new_memory #[B, MEM_SLOTS, MEM_SIZE]\n",
    "    \n",
    "    \n",
    "    def _attend_over_memory(self, memory):\n",
    "        # memory: [B, MEM_SLOT, MEM_SIZE]\n",
    "        for _ in range(self._num_blocks):\n",
    "            attended_memory = self._multihead_attention(memory) # [B, MEM_SLOT, MEM_SIZE]\n",
    "            \n",
    "            # add a skip connection the multiheaded attention's input.\n",
    "            # memory = LN_1(memory + attended_memory) [B*MEM_SLOT, MEM_SIZE]\n",
    "            memory = self._attend_layer_norm_1((memory + attended_memory).view(-1, memory.size(2)))\n",
    "            \n",
    "            # add a skip connection to the attention_mlp's input.\n",
    "            # memory = LN_2( MLP(memory) + memory)\n",
    "            memory = self._attend_layer_norm_2(self._attention_mlp(memory) + memory).view(-1, \n",
    "                                                                                          attended_memory.size(1),\n",
    "                                                                                          attended_memory.size(2))\n",
    "        return memory\n",
    "    \n",
    "    def _calculate_gate_size(self):\n",
    "        if self._gate_style == 'unit':\n",
    "            return self._mem_size\n",
    "        elif self._gate_style == 'memory':\n",
    "          return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def _create_gates(self, inputs, memory):\n",
    "        hidden = torch.tanh(memory)\n",
    "        \n",
    "        #inputs [B, 1, MEM_SIZE] -> [B, 1*MEM_SIZE]\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        \n",
    "        # [B, 1*MEM_SIZE] -> Linear -> [B, num_gates] -> [B, 1, num_gates]\n",
    "        gate_inputs = self._gate_inputs_linear(inputs).unsqueeze(1)\n",
    "        \n",
    "        # hidden [B, MEM_SLOT, MEM_SIZE] -> [B*MEM_SLOT, MEM_SIZE] -> Linear -> [B*MEM_SLOT, num_gates]\n",
    "        # -> [B, MEM_SLOT, num_gates]\n",
    "        gate_hidden = self._gate_hidden_linear(hidden.view(-1, hidden.size(2))).view(hidden.size(0),\n",
    "                                                                                     hidden.size(1),\n",
    "                                                                                     -1)\n",
    "        \n",
    "        input_gate, forget_gate = torch.chunk(gate_hidden + gate_inputs, 2, dim=2)\n",
    "        \n",
    "        input_gate = torch.sigmoid(input_gate + self._input_bias)\n",
    "        forget_gate = torch.sigmoid(forget_gate + self._forget_bias)\n",
    "        \n",
    "        return input_gate, forget_gate #[B, MEM_SLOT, num_gates/2], [B, MEM_SLOT, num_gates/2]\n",
    "                                                   \n",
    "    def forward(self, x, memory=None, treat_input_as_matrix=False):\n",
    "        # x: [B, T, F=input_size]\n",
    "        # memory: [B, MEM_SLOTS, MEM_SIZE]\n",
    "        batch_size = x.size(0)\n",
    "        total_timesteps = x.size(1)\n",
    "        \n",
    "        output_accumulator = x.new_zeros(batch_size, total_timesteps, self._mem_slots*self._mem_size)\n",
    "        \n",
    "        for index in range(total_timesteps):\n",
    "            # For each time-step\n",
    "            # inputs: [B, 1, F=input_size]\n",
    "            inputs = x[:,index].unsqueeze(1)\n",
    "            \n",
    "            if treat_input_as_matrix:\n",
    "                # [B, 1, F] -> [B*1, F] -> linear ->[B*1, MEM_SIZE] -> [B, 1, MEM_SIZE]\n",
    "                inputs_reshape =  self._linear(inputs.view(-1, inputs.size(2))).view(inputs.size(0), \n",
    "                                                                                     -1, \n",
    "                                                                                     self._mem_size)\n",
    "            else:\n",
    "                # [B, 1, F] -> [B, 1*F] -> linear -> [B, MEM_SIZE] -> [B, 1, MEM_SIZE]\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                inputs = self._linear(inputs)\n",
    "                inputs_reshape = inputs.unsqueeze(1)\n",
    "\n",
    "            # [B, MEM_SLOTS, MEM_SIZE] -> [B, MEM_SLOT+1, MEM_SIZE]\n",
    "            memory_plus_input = torch.cat([memory, inputs_reshape], dim=1)\n",
    "\n",
    "            next_memory = self._attend_over_memory(memory_plus_input)\n",
    "            n = inputs_reshape.size(1)\n",
    "            # [B, MEM_SLOT+1, MEM_SIZE] -> [B, MEM_SLOT, MEM_SIZE]\n",
    "            next_memory = next_memory[:, :-n, :]\n",
    "\n",
    "            if self._gate_style == 'unit' or self._gate_style == 'memory':\n",
    "                input_gate, forget_gate = self._create_gates(inputs_reshape, memory) #[B, MEM_SLOT, num_gates/2] \n",
    "                next_memory = input_gate * torch.tanh(next_memory)\n",
    "                next_memory += forget_gate * memory\n",
    "            \n",
    "            # output: [B, MEM_SLOT, MEM_SIZE] -> [B, MEM_SLOT*MEM_SIZE]\n",
    "            output = next_memory.view(next_memory.size(0), -1)\n",
    "            \n",
    "            output_accumulator[:,index] = output\n",
    "            memory = next_memory\n",
    "        \n",
    "        return output_accumulator, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NthFarthest(Dataset):\n",
    "    def __init__(self, num_objects, num_features, batch_size, epochs, transform=None, target_transform=None):\n",
    "        super(NthFarthest, self).__init__()\n",
    "        \n",
    "        self._num_objects = num_objects\n",
    "        self._num_features = num_features\n",
    "        self._transform = transform\n",
    "        self._target_transform = target_transform\n",
    "        \n",
    "    def _get_single_set(self, num_objects, num_features):\n",
    "        # Generate random binary vectors\n",
    "        data = np.random.uniform(-1, 1, size=(num_objects, num_features))\n",
    "\n",
    "        distances = spdistance.squareform(spdistance.pdist(data))\n",
    "        distance_idx = np.argsort(distances)\n",
    "\n",
    "        # Choose random distance\n",
    "        nth = np.random.randint(0, num_objects)\n",
    "\n",
    "        # Pick out the nth furthest for each object\n",
    "        nth_furthest = np.where(distance_idx == nth)[1]\n",
    "\n",
    "        # Choose random reference object\n",
    "        reference = np.random.randint(0, num_objects)\n",
    "\n",
    "        # Get identity of object that is the nth furthest from reference object\n",
    "        labels = nth_furthest[reference]\n",
    "\n",
    "        # Compile data\n",
    "        object_ids = np.identity(num_objects)\n",
    "        nth_matrix = np.zeros((num_objects, num_objects))\n",
    "        nth_matrix[:, nth] = 1\n",
    "        reference_object = np.zeros((num_objects, num_objects))\n",
    "        reference_object[:, reference] = 1\n",
    "\n",
    "        inputs = np.concatenate([data, object_ids, reference_object, nth_matrix],\n",
    "                                axis=-1)\n",
    "        inputs = np.random.permutation(inputs)\n",
    "        labels = np.expand_dims(labels, axis=0)\n",
    "        return inputs.astype(np.float32), labels.astype(np.long)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inputs, labels = self._get_single_set(self._num_objects, self._num_features)\n",
    "        \n",
    "        if self._transform is not None:\n",
    "            inputs = self._transform(inputs)\n",
    "        if self._target_transform is not None:\n",
    "            labels = self._target_transform(labels)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return batch_size*epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_slots = 4\n",
    "head_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1600\n",
    "epochs = 1000000\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "num_objects = 4\n",
    "num_features = 4\n",
    "input_size = num_features + 3 * num_objects\n",
    "\n",
    "mlp_size = 256\n",
    "mlp_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_furthest = NthFarthest(num_objects=num_objects,\n",
    "                         num_features=num_features, \n",
    "                         batch_size=batch_size, \n",
    "                         epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(batch_size=batch_size, \n",
    "                                           dataset=n_furthest)\n",
    "test_loader = torch.utils.data.DataLoader(batch_size=batch_size,\n",
    "                                          dataset=n_furthest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, input_size, mem_slots, head_size, batch_size,\n",
    "                 mlp_size, mlp_layers, num_objects):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        \n",
    "        self._core = RelationalMemory(input_size=input_size,\n",
    "                                      mem_slots=mem_slots, \n",
    "                                      head_size=head_size)\n",
    "        self.initial_memory = self._core.initial_state(batch_size=batch_size)\n",
    "        \n",
    "        final_mlp_modules = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.Linear(in_features=self._core._mem_size * self._core._mem_slots,\n",
    "                          out_features=mlp_size),\n",
    "                nn.ReLU())] +\n",
    "            [nn.Sequential(\n",
    "                nn.Linear(in_features=mlp_size,\n",
    "                          out_features=mlp_size),\n",
    "                nn.ReLU())] * (mlp_layers - 2) +\n",
    "            [nn.Linear(in_features=mlp_size,\n",
    "                       out_features=mlp_size)]\n",
    "        )\n",
    "        self._final_mlp = nn.Sequential(*final_mlp_modules)\n",
    "        \n",
    "        self._linear = nn.Linear(in_features=mlp_size,\n",
    "                                 out_features=num_objects)\n",
    "        \n",
    "    # inputs: [B, T, F]\n",
    "    def forward(self, inputs, memory):\n",
    "        output_sequence, output_memory = self._core(inputs, memory)\n",
    "        outputs = output_sequence[:, -1, :].unsqueeze(1)\n",
    "        \n",
    "        outputs = self._final_mlp(outputs)\n",
    "        logits = self._linear(outputs)\n",
    "        \n",
    "        return logits, output_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceModel(input_size=input_size,\n",
    "                      mem_slots=mem_slots, \n",
    "                      head_size=head_size, \n",
    "                      batch_size=batch_size,\n",
    "                      mlp_size=mlp_size, \n",
    "                      mlp_layers=mlp_layers, \n",
    "                      num_objects=num_objects).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  1.3867828845977783\n",
      "loss:  1.388818621635437\n",
      "loss:  1.388674259185791\n",
      "loss:  1.3847616910934448\n",
      "loss:  1.3875209093093872\n",
      "loss:  1.3877465724945068\n",
      "loss:  1.386612892150879\n",
      "loss:  1.3863935470581055\n",
      "loss:  1.387014627456665\n",
      "loss:  1.3877519369125366\n",
      "loss:  1.3856565952301025\n",
      "loss:  1.388579249382019\n",
      "loss:  1.388862133026123\n",
      "loss:  1.3882534503936768\n",
      "loss:  1.3880913257598877\n",
      "loss:  1.3872520923614502\n",
      "loss:  1.3843721151351929\n",
      "loss:  1.386006474494934\n",
      "loss:  1.3887149095535278\n",
      "loss:  1.3873852491378784\n",
      "loss:  1.3886222839355469\n",
      "loss:  1.3893730640411377\n",
      "loss:  1.3872851133346558\n",
      "loss:  1.3868238925933838\n",
      "loss:  1.3878239393234253\n",
      "loss:  1.3880730867385864\n",
      "loss:  1.3896894454956055\n",
      "loss:  1.3878532648086548\n",
      "loss:  1.3880445957183838\n",
      "loss:  1.3889265060424805\n",
      "loss:  1.3878422975540161\n",
      "loss:  1.387606143951416\n",
      "loss:  1.3861229419708252\n",
      "loss:  1.3868069648742676\n",
      "loss:  1.389318585395813\n",
      "loss:  1.388189673423767\n",
      "loss:  1.3883622884750366\n",
      "loss:  1.3864812850952148\n",
      "loss:  1.388676404953003\n",
      "loss:  1.382554531097412\n",
      "loss:  1.3894709348678589\n",
      "loss:  1.3858675956726074\n",
      "loss:  1.3871639966964722\n",
      "loss:  1.3882429599761963\n",
      "loss:  1.3864856958389282\n",
      "loss:  1.386618971824646\n",
      "loss:  1.38799250125885\n",
      "loss:  1.3890044689178467\n",
      "loss:  1.3878344297409058\n",
      "loss:  1.3862556219100952\n",
      "loss:  1.3886747360229492\n",
      "loss:  1.388045072555542\n",
      "loss:  1.389732837677002\n",
      "loss:  1.3887006044387817\n",
      "loss:  1.3855737447738647\n",
      "loss:  1.3880504369735718\n",
      "loss:  1.3885008096694946\n",
      "loss:  1.3881475925445557\n",
      "loss:  1.38682222366333\n",
      "loss:  1.3866020441055298\n",
      "loss:  1.3837366104125977\n",
      "loss:  1.3874516487121582\n",
      "loss:  1.3884202241897583\n",
      "loss:  1.3872343301773071\n",
      "loss:  1.3853652477264404\n",
      "loss:  1.3880531787872314\n",
      "loss:  1.3866766691207886\n",
      "loss:  1.3901194334030151\n",
      "loss:  1.389215350151062\n",
      "loss:  1.3881645202636719\n",
      "loss:  1.3878226280212402\n",
      "loss:  1.3882657289505005\n",
      "loss:  1.3849443197250366\n",
      "loss:  1.3859339952468872\n",
      "loss:  1.387367844581604\n",
      "loss:  1.3877350091934204\n",
      "loss:  1.387606143951416\n",
      "loss:  1.385066270828247\n",
      "loss:  1.3893758058547974\n",
      "loss:  1.387975811958313\n",
      "loss:  1.3850539922714233\n",
      "loss:  1.3880997896194458\n",
      "loss:  1.3881103992462158\n",
      "loss:  1.3868136405944824\n",
      "loss:  1.3840112686157227\n",
      "loss:  1.389797568321228\n",
      "loss:  1.3864892721176147\n",
      "loss:  1.385872483253479\n",
      "loss:  1.3897978067398071\n",
      "loss:  1.391034722328186\n",
      "loss:  1.3858658075332642\n",
      "loss:  1.3883471488952637\n",
      "loss:  1.3877792358398438\n",
      "loss:  1.3879846334457397\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6b063778391a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-162d0573e375>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, memory)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# inputs: [B, T, F]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moutput_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_sequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1e8a0dca20e6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, memory, treat_input_as_matrix)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mmemory_plus_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_reshape\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m             \u001b[0mnext_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attend_over_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_plus_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_reshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# [B, MEM_SLOT+1, MEM_SIZE] -> [B, MEM_SLOT, MEM_SIZE]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1e8a0dca20e6>\u001b[0m in \u001b[0;36m_attend_over_memory\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# memory: [B, MEM_SLOT, MEM_SIZE]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mattended_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multihead_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [B, MEM_SLOT, MEM_SIZE]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# add a skip connection the multiheaded attention's input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1e8a0dca20e6>\u001b[0m in \u001b[0;36m_multihead_attention\u001b[0;34m(self, memory)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# [B, MEM_SLOT, MEM_SIZE] -> [B*MEM_SLOT, MEM_SIZE] -> Linear -> [B*MEM_SLOT, F]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mqkv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attention_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# [B*MEM_SLOT, F] -> Layer Norm -> [B*MEM_SLOT, F] -> [B, MEM_SLOT, F]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1041\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "memory = model.initial_memory\n",
    "\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    optimiser.zero_grad()\n",
    "    output, output_memory = model(inputs, memory)\n",
    "    \n",
    "    loss = criterion(output.squeeze(1), labels.squeeze(1))\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"loss: \", loss.cpu().item())\n",
    "    \n",
    "    memory = output_memory.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
