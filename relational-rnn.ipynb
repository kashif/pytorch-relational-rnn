{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational RNNs by Adam Santoro et al. in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, PackedSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_size: \n",
    "        mem_slots: The total number of memory slots to use.\n",
    "        head_size: The size of an attention head.\n",
    "        num_heads: The number of attention heads to use. Defaults to 1.\n",
    "        num_blocks: Number of times to compute attention per time step. Defaults to 1.\n",
    "        forget_bias:\n",
    "        input_bias:\n",
    "        gate_style:\n",
    "        attention_mlp_layers:\n",
    "        key_size:\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size,\n",
    "                 mem_slots, head_size, num_heads=1, num_blocks=1,\n",
    "                 forget_bias=1.0, input_bias=0.0, gate_style='unit',\n",
    "                 attention_mlp_layers=2, key_size=None):\n",
    "        super(RelationalMemory, self).__init__()\n",
    "        \n",
    "        self._mem_slots = mem_slots\n",
    "        self._head_size = head_size\n",
    "        self._num_heads = num_heads\n",
    "        self._mem_size = self._head_size * self._num_heads # hidden_size\n",
    "\n",
    "        if num_blocks < 1:\n",
    "            raise ValueError('num_blocks must be >= 1. Got: {}.'.format(num_blocks))\n",
    "        self._num_blocks = num_blocks\n",
    "\n",
    "        self._forget_bias = forget_bias\n",
    "        self._input_bias = input_bias\n",
    "\n",
    "        if gate_style not in ['unit', 'memory', None]:\n",
    "            raise ValueError(\n",
    "                'gate_style must be one of [\\'unit\\', \\'memory\\', None]. Got: '\n",
    "                '{}.'.format(gate_style))\n",
    "        self._gate_style = gate_style\n",
    "\n",
    "        if attention_mlp_layers < 1:\n",
    "            raise ValueError('attention_mlp_layers must be >= 1. Got: {}.'.format(\n",
    "                attention_mlp_layers))\n",
    "        self._attention_mlp_layers = attention_mlp_layers\n",
    "\n",
    "        self._key_size = key_size if key_size else self._head_size\n",
    "\n",
    "        self._linear = nn.Linear(in_features=input_size,\n",
    "                                 out_features=self._mem_size)\n",
    "        \n",
    "        key_size = self._key_size\n",
    "        value_size = self._head_size\n",
    "        qkv_size = 2 * key_size + value_size\n",
    "        total_size = qkv_size * self._num_heads\n",
    "        self._attention_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                           out_features=total_size)\n",
    "        self._attention_layer_norm = nn.LayerNorm(total_size)\n",
    "        \n",
    "        self._attention_mlp = \n",
    "        self._attend_layer_norm_1 = nn.LayerNorm()\n",
    "        self._attend_layer_norm_2 = nn.LayerNorm()\n",
    "        \n",
    "        \n",
    " \n",
    "        \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\"Creates the initial memory.\n",
    "\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size (self._batch_size, self._mem_slots, self._mem_size).\n",
    "\n",
    "        Returns:\n",
    "            init_state: A truncated or padded matrix of size\n",
    "            (self._batch_size, self._mem_slots, self._mem_size).\n",
    "        \"\"\"    \n",
    "        init_state = torch.eye(num_rows=self._mem_slots).repeat(self._batch_size, 1, 1)\n",
    "        \n",
    "        # Pad the matrix with zeros.\n",
    "        if self._mem_size > self._mem_slots:\n",
    "            difference = self._mem_size - self._mem_slots\n",
    "            pad = torch.zeros((self._batch_size, self._mem_slots, difference))\n",
    "            init_state = torch.cat([init_state, pad], dim=-1)\n",
    "        # Truncation. Take the first `self._mem_size` components.\n",
    "        elif self._mem_size < self._mem_slots:\n",
    "            init_state = init_state[:, :, :self._mem_size]\n",
    "        \n",
    "        return init_state\n",
    "        \n",
    "\n",
    "    def _multihead_attention(self, memory): # memory: [B, MEM_SLOT, MEM_SIZE]\n",
    "        # F = total_size\n",
    "        # mem_slots = MEM_SLOT = N\n",
    "        mem_slots = memory.size(1)\n",
    "        \n",
    "        # [B, MEM_SLOT, MEM_SIZE] -> [B*MEM_SLOT, MEM_SIZE] -> Linear -> [B*MEM_SLOT, F]\n",
    "        qkv = self._attention_linear(memory.view(-1, memory.size(2)))\n",
    "        \n",
    "        # [B*MEM_SLOT, F] -> Layer Norm -> [B*MEM_SLOT, F] -> [B, MEM_SLOT, F]\n",
    "        qkv = self._attention_layer_norm(qkv).view(memory.size(0), mem_slots, -1)\n",
    "        \n",
    "        # H = num_heads\n",
    "        qkv_size = 2 * self._key_size + self._head_size\n",
    "        \n",
    "        # [B, N, F] -> [B, N, H, F/H]\n",
    "        qkv_reshape = qkv.view(-1, mem_slots, self._num_heads, qkv_size)\n",
    "        \n",
    "        # [B, N, H, F/H] -> [B, H, N, F/H]\n",
    "        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n",
    "        # split q, k, v\n",
    "        q, k, v = torch.split(qkv_transpose, [self._key_size, self._key_size, self._head_size], dim=-1)\n",
    "        \n",
    "        q *= qkv_size ** -0.5\n",
    "        dot_product = torch.matmul(q, torch.t(k)) # [B, H, N, N]\n",
    "        weights = F.softmax(dot_product)\n",
    "        \n",
    "        #[B, H, N, V]\n",
    "        output = torch.matmul(weights, v)\n",
    "        \n",
    "        # [B, H, N, V] -> [B, N, H, V]\n",
    "        output_transpose = output.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # [B, N, H, V] -> [B, N, H * V]\n",
    "        new_memory = output_transpose.view(-1, output_transpose.size(1), \n",
    "                                           output_transpose.size(2)*output_transpose.size(3))\n",
    "        \n",
    "        return new_memory #[B, MEM_SLOTS, MEM_SIZE]\n",
    "    \n",
    "    \n",
    "    def _attend_over_memory(self, memory):\n",
    "        # memory: [B, MEM_SLOT, MEM_SIZE]\n",
    "        for _ in range(self._num_blocks):\n",
    "            attended_memory = self._multihead_attention(memory) # [B, MEM_SLOT, MEM_SIZE]\n",
    "            \n",
    "            # add a skip connection the multiheaded attention's input.\n",
    "            memory = self.LayerNorm((memory + attended_memory).view(-1, memory.size(2))).view(memory.size(0),\n",
    "                                                                                              memory.size(1), -1)\n",
    "\n",
    "            # add a skip connection to the attention_mlp's input.\n",
    "            \n",
    "            \n",
    "        return memory\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x, memory, treat_input_as_matrix=False):\n",
    "        # x: [B, T, F=input_size]\n",
    "        batch_size = x.size(0)\n",
    "        total_timesteps = x.size(1)\n",
    "        \n",
    "        for index in range(total_timesteps):\n",
    "            # For each time-step\n",
    "            # inputs: [B, 1, F]\n",
    "            inputs = x[:,index].unsqueeze(1)\n",
    "            \n",
    "            # memory: [B, MEM_SLOTS, MEM_SIZE]\n",
    "            if treat_input_as_matrix:\n",
    "                # [B, 1, F] -> [B*1, F] -> linear ->[B*1, MEM_SIZE] -> [B, 1, MEM_SIZE]\n",
    "                inputs_reshape =  self._linear(inputs.view(-1, input.size(2))).view(input.size(0), -1, self._mem_size)\n",
    "            else:\n",
    "                # [B, 1, F] -> [B, 1*F] -> linear -> [B, MEM_SIZE] -> [B, 1, MEM_SIZE]\n",
    "                inputs = inputs.view(input.size(0), -1)\n",
    "                inputs = self._linear(inputs)\n",
    "                input_reshape = inputs.unsqueze(1)\n",
    "\n",
    "            # [B, MEM_SLOTS, MEM_SIZE] -> [B, MEM_SLOT+1 or MEM_SLOT+1, MEM_SIZE]\n",
    "            memory_plus_input = torch.cat([memory, input_reshape], dim=1)\n",
    "\n",
    "            next_memory = self._attend_over_memory(memory_plus_input)\n",
    "            n = inputs_reshape.size(1)\n",
    "            # [B, MEM_SLOT+1, MEM_SIZE] -> [B, MEM_SLOT, MEM_SIZE]\n",
    "            next_memory = next_memory[:, :-n, :]\n",
    "\n",
    "            if self._gate_style == 'unit' or self._gate_style == 'memory':\n",
    "                self._input_gate, self._forget_gate = self._create_gates(inputs_reshape, memory)\n",
    "                next_memory = self._input_gate * tf.tanh(next_memory)\n",
    "                next_memory += self._forget_gate * memory\n",
    "\n",
    "            output = \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "insputs = torch.Tensor(64, 10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "insputs[:, 9].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "from sonnet.python.modules import relational_memory\n",
    "from sonnet.python.modules import basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_slots = 4\n",
    "head_size = 32\n",
    "num_heads = 2\n",
    "batch_size = 5\n",
    "\n",
    "input_shape = (batch_size, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = relational_memory.RelationalMemory(mem_slots, head_size, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = mem.initial_state(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5), Dimension(4), Dimension(64)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = basic.BatchFlatten(preserve_dims=2)(tf.zeros(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5), Dimension(1), Dimension(3)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_reshape = basic.BatchApply(basic.Linear(64), n_dims=2)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(5), Dimension(1), Dimension(64)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = inputs_reshape.get_shape().as_list()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
