{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational RNNs by Adam Santoro et al. in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance as spdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_size: The size of the input features\n",
    "        mem_slots: The total number of memory slots to use.\n",
    "        head_size: The size of an attention head.\n",
    "        num_heads: The number of attention heads to use. Defaults to 1.\n",
    "        num_blocks: Number of times to compute attention per time step. Defaults to 1.\n",
    "        forget_bias: \n",
    "        input_bias:\n",
    "        gate_style:\n",
    "        attention_mlp_layers:\n",
    "        key_size:\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size,\n",
    "                 mem_slots, head_size, num_heads=1, num_blocks=1,\n",
    "                 forget_bias=1.0, input_bias=0.0, gate_style='unit',\n",
    "                 attention_mlp_layers=2, key_size=None):\n",
    "        super(RelationalMemory, self).__init__()\n",
    "        \n",
    "        self._mem_slots = mem_slots\n",
    "        self._head_size = head_size\n",
    "        self._num_heads = num_heads\n",
    "        self._mem_size = self._head_size * self._num_heads\n",
    "\n",
    "        if num_blocks < 1:\n",
    "            raise ValueError('num_blocks must be >= 1. Got: {}.'.format(num_blocks))\n",
    "        self._num_blocks = num_blocks\n",
    "\n",
    "        self._forget_bias = forget_bias\n",
    "        self._input_bias = input_bias\n",
    "\n",
    "        if gate_style not in ['unit', 'memory', None]:\n",
    "            raise ValueError(\n",
    "                'gate_style must be one of [\\'unit\\', \\'memory\\', None]. Got: '\n",
    "                '{}.'.format(gate_style))\n",
    "        self._gate_style = gate_style\n",
    "\n",
    "        if attention_mlp_layers < 1:\n",
    "            raise ValueError('attention_mlp_layers must be >= 1. Got: {}.'.format(\n",
    "                attention_mlp_layers))\n",
    "        self._attention_mlp_layers = attention_mlp_layers\n",
    "\n",
    "        self._key_size = key_size if key_size else self._head_size\n",
    "\n",
    "        self._linear = nn.Linear(in_features=input_size,\n",
    "                                 out_features=self._mem_size)\n",
    "        \n",
    "        self.qkv_size = 2 * self._key_size + self._head_size\n",
    "        total_size = self.qkv_size * self._num_heads\n",
    "        self._attention_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                           out_features=total_size)\n",
    "        self._attention_layer_norm = nn.LayerNorm(total_size)\n",
    "        \n",
    "        attention_mlp_modules = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(in_features=self._mem_size,\n",
    "                          out_features=self._mem_size),\n",
    "                nn.ReLU())] * (self._attention_mlp_layers - 1) +\n",
    "            [nn.Linear(in_features=self._mem_size,\n",
    "                       out_features=self._mem_size)]\n",
    "        )\n",
    "        self._attention_mlp = nn.Sequential(*attention_mlp_modules)\n",
    "        \n",
    "        self._attend_layer_norm_1 = nn.LayerNorm(self._mem_size)\n",
    "        self._attend_layer_norm_2 = nn.LayerNorm(self._mem_size)\n",
    "        \n",
    "        num_gates = 2 * self._calculate_gate_size()\n",
    "        self._gate_inputs_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                             out_features=num_gates)\n",
    "        \n",
    "        self._gate_hidden_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                             out_features=num_gates)\n",
    "        \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\"Creates the initial memory.\n",
    "\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size \n",
    "        (batch_size, self._mem_slots, self._mem_size).\n",
    "\n",
    "        Returns:\n",
    "            init_state: A truncated or padded matrix of size\n",
    "            (batch_size, self._mem_slots, self._mem_size).\n",
    "        \"\"\"    \n",
    "        init_state = torch.eye(n=self._mem_slots).repeat(batch_size, 1, 1)\n",
    "        \n",
    "        if self._mem_size > self._mem_slots:\n",
    "            # Pad the matrix with zeros.\n",
    "            difference = self._mem_size - self._mem_slots\n",
    "            pad = torch.zeros((batch_size, self._mem_slots, difference))\n",
    "            init_state = torch.cat([init_state, pad], dim=-1)\n",
    "        elif self._mem_size < self._mem_slots:\n",
    "            # Truncation. Take the first `self._mem_size` components.\n",
    "            init_state = init_state[:, :, :self._mem_size]\n",
    "        \n",
    "        return init_state.detach()\n",
    "\n",
    "    def _multihead_attention(self, memory): # memory: [B, MEM_SLOT, MEM_SIZE]\n",
    "        # F = total_size\n",
    "        # mem_slots = MEM_SLOT = N\n",
    "        mem_slots = memory.size(1)\n",
    "        \n",
    "        # [B, MEM_SLOT, MEM_SIZE] -> [B*MEM_SLOT, MEM_SIZE] -> Linear -> [B*MEM_SLOT, F]\n",
    "        qkv = self._attention_linear(memory.view(-1, memory.size(2)))\n",
    "        \n",
    "        # [B*MEM_SLOT, F] -> Layer Norm -> [B*MEM_SLOT, F] -> [B, MEM_SLOT, F]\n",
    "        qkv = self._attention_layer_norm(qkv).view(memory.size(0), mem_slots, -1)\n",
    "        \n",
    "        # H = num_heads\n",
    "        #qkv_size = 2 * self._key_size + self._head_size\n",
    "        # [B, N, F] -> [B, N, H, F/H]\n",
    "        qkv_reshape = qkv.view(-1, mem_slots, self._num_heads, self.qkv_size)\n",
    "        \n",
    "        # [B, N, H, F/H] -> [B, H, N, F/H]\n",
    "        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n",
    "        # split q, k, v\n",
    "        q, k, v = torch.split(qkv_transpose, [self._key_size, self._key_size, self._head_size], dim=-1)\n",
    "        \n",
    "        q *= self._key_size ** -0.5\n",
    "        dot_product = torch.matmul(q, torch.transpose(k, 2, 3)) # [B, H, N, N]\n",
    "        weights = F.softmax(dot_product, dim=-1)\n",
    "        \n",
    "        #[B, H, N, V]\n",
    "        output = torch.matmul(weights, v)\n",
    "        \n",
    "        # [B, H, N, V] -> [B, N, H, V]\n",
    "        output_transpose = output.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # [B, N, H, V] -> [B, N, H * V]\n",
    "        new_memory = output_transpose.contiguous().view(-1, output_transpose.size(1), \n",
    "                                                        output_transpose.size(2)*output_transpose.size(3))\n",
    "        \n",
    "        return new_memory #[B, MEM_SLOTS, MEM_SIZE]\n",
    "    \n",
    "    \n",
    "    def _attend_over_memory(self, memory):\n",
    "        # memory: [B, MEM_SLOT, MEM_SIZE]\n",
    "        for _ in range(self._num_blocks):\n",
    "            attended_memory = self._multihead_attention(memory) # [B, MEM_SLOT, MEM_SIZE]\n",
    "            \n",
    "            # add a skip connection the multiheaded attention's input.\n",
    "            # memory = LN_1(memory + attended_memory) [B*MEM_SLOT, MEM_SIZE]\n",
    "            memory = self._attend_layer_norm_1((memory + attended_memory).view(-1, memory.size(2)))\n",
    "            \n",
    "            # add a skip connection to the attention_mlp's input.\n",
    "            # memory = LN_2( MLP(memory) + memory)\n",
    "            memory = self._attend_layer_norm_2(self._attention_mlp(memory) + memory).view(-1, \n",
    "                                                                                          attended_memory.size(1),\n",
    "                                                                                          attended_memory.size(2))\n",
    "        return memory\n",
    "    \n",
    "    def _calculate_gate_size(self):\n",
    "        if self._gate_style == 'unit':\n",
    "            return self._mem_size\n",
    "        elif self._gate_style == 'memory':\n",
    "          return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def _create_gates(self, inputs, memory):\n",
    "        hidden = torch.tanh(memory)\n",
    "        \n",
    "        #inputs [B, 1, MEM_SIZE] -> [B, 1*MEM_SIZE]\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        \n",
    "        # [B, 1*MEM_SIZE] -> Linear -> [B, num_gates] -> [B, 1, num_gates]\n",
    "        gate_inputs = self._gate_inputs_linear(inputs).unsqueeze(1)\n",
    "        \n",
    "        # hidden [B, MEM_SLOT, MEM_SIZE] -> [B*MEM_SLOT, MEM_SIZE] -> Linear -> [B*MEM_SLOT, num_gates]\n",
    "        # -> [B, MEM_SLOT, num_gates]\n",
    "        gate_hidden = self._gate_hidden_linear(hidden.view(-1, hidden.size(2))).view(hidden.size(0),\n",
    "                                                                                     hidden.size(1),\n",
    "                                                                                     -1)\n",
    "        \n",
    "        input_gate, forget_gate = torch.chunk(gate_hidden + gate_inputs, 2, dim=2)\n",
    "        \n",
    "        input_gate = torch.sigmoid(input_gate + self._input_bias)\n",
    "        forget_gate = torch.sigmoid(forget_gate + self._forget_bias)\n",
    "        \n",
    "        return input_gate, forget_gate #[B, MEM_SLOT, num_gates/2], [B, MEM_SLOT, num_gates/2]\n",
    "                                                   \n",
    "    def forward(self, x, memory=None, treat_input_as_matrix=False):\n",
    "        # x: [B, T, F=input_size]\n",
    "        # memory: [B, MEM_SLOTS, MEM_SIZE]\n",
    "        batch_size = x.size(0)\n",
    "        total_timesteps = x.size(1)\n",
    "        \n",
    "        output_accumulator = x.new_zeros(batch_size, total_timesteps, self._mem_slots*self._mem_size)\n",
    "        \n",
    "        for index in range(total_timesteps):\n",
    "            # For each time-step\n",
    "            # inputs: [B, 1, F=input_size]\n",
    "            inputs = x[:,index].unsqueeze(1)\n",
    "            \n",
    "            if treat_input_as_matrix:\n",
    "                # [B, 1, F] -> [B*1, F] -> linear ->[B*1, MEM_SIZE] -> [B, 1, MEM_SIZE]\n",
    "                inputs_reshape =  self._linear(inputs.view(-1, inputs.size(2))).view(inputs.size(0), \n",
    "                                                                                     -1, \n",
    "                                                                                     self._mem_size)\n",
    "            else:\n",
    "                # [B, 1, F] -> [B, 1*F] -> linear -> [B, 1*MEM_SIZE] -> [B, 1, MEM_SIZE]\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                inputs = self._linear(inputs)\n",
    "                inputs_reshape = inputs.unsqueeze(1)\n",
    "\n",
    "            # [B, MEM_SLOTS, MEM_SIZE] -> [B, MEM_SLOT+1, MEM_SIZE]\n",
    "            memory_plus_input = torch.cat([memory, inputs_reshape], dim=1)\n",
    "\n",
    "            next_memory = self._attend_over_memory(memory_plus_input)\n",
    "            n = inputs_reshape.size(1)\n",
    "            # [B, MEM_SLOT+1, MEM_SIZE] -> [B, MEM_SLOT, MEM_SIZE]\n",
    "            next_memory = next_memory[:, :-n, :]\n",
    "\n",
    "            if self._gate_style == 'unit' or self._gate_style == 'memory':\n",
    "                input_gate, forget_gate = self._create_gates(inputs_reshape, memory) #[B, MEM_SLOT, num_gates/2] \n",
    "                next_memory = input_gate * torch.tanh(next_memory)\n",
    "                next_memory += forget_gate * memory\n",
    "            \n",
    "            # output: [B, MEM_SLOT, MEM_SIZE] -> [B, MEM_SLOT*MEM_SIZE]\n",
    "            output = next_memory.view(next_memory.size(0), -1)\n",
    "            \n",
    "            output_accumulator[:,index] = output\n",
    "            memory = next_memory\n",
    "        \n",
    "        return output_accumulator, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NthFarthest(Dataset):\n",
    "    def __init__(self, num_objects, num_features, batch_size, epochs, transform=None, target_transform=None):\n",
    "        super(NthFarthest, self).__init__()\n",
    "        \n",
    "        self._num_objects = num_objects\n",
    "        self._num_features = num_features\n",
    "        self._transform = transform\n",
    "        self._target_transform = target_transform\n",
    "        \n",
    "    def _get_single_set(self, num_objects, num_features):\n",
    "        # Generate random binary vectors\n",
    "        data = np.random.uniform(-1, 1, size=(num_objects, num_features))\n",
    "\n",
    "        distances = spdistance.squareform(spdistance.pdist(data))\n",
    "        distance_idx = np.argsort(distances)\n",
    "\n",
    "        # Choose random distance\n",
    "        nth = np.random.randint(0, num_objects)\n",
    "\n",
    "        # Pick out the nth furthest for each object\n",
    "        nth_furthest = np.where(distance_idx == nth)[1]\n",
    "\n",
    "        # Choose random reference object\n",
    "        reference = np.random.randint(0, num_objects)\n",
    "\n",
    "        # Get identity of object that is the nth furthest from reference object\n",
    "        labels = nth_furthest[reference]\n",
    "\n",
    "        # Compile data\n",
    "        object_ids = np.identity(num_objects)\n",
    "        nth_matrix = np.zeros((num_objects, num_objects))\n",
    "        nth_matrix[:, nth] = 1\n",
    "        reference_object = np.zeros((num_objects, num_objects))\n",
    "        reference_object[:, reference] = 1\n",
    "\n",
    "        inputs = np.concatenate([data, object_ids, reference_object, nth_matrix],\n",
    "                                axis=-1)\n",
    "        inputs = np.random.permutation(inputs)\n",
    "        labels = np.expand_dims(labels, axis=0)\n",
    "        return inputs.astype(np.float32), labels.astype(np.long)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        inputs, labels = self._get_single_set(self._num_objects, self._num_features)\n",
    "        \n",
    "        if self._transform is not None:\n",
    "            inputs = self._transform(inputs)\n",
    "        if self._target_transform is not None:\n",
    "            labels = self._target_transform(labels)\n",
    "\n",
    "        return inputs, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return batch_size*epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_slots = 4\n",
    "head_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 1000000\n",
    "\n",
    "learning_rate = 1e-3\n",
    "\n",
    "num_objects = 2\n",
    "num_features = 2\n",
    "input_size = num_features + 3 * num_objects\n",
    "\n",
    "mlp_size = 256\n",
    "mlp_layers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_furthest = NthFarthest(num_objects=num_objects,\n",
    "                         num_features=num_features, \n",
    "                         batch_size=batch_size, \n",
    "                         epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(batch_size=batch_size, \n",
    "                                           dataset=n_furthest)\n",
    "test_loader = torch.utils.data.DataLoader(batch_size=batch_size,\n",
    "                                          dataset=n_furthest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "    def __init__(self, input_size, mem_slots, head_size, batch_size,\n",
    "                 mlp_size, mlp_layers, num_objects):\n",
    "        super(SequenceModel, self).__init__()\n",
    "        \n",
    "        self._core = RelationalMemory(input_size=input_size,\n",
    "                                      mem_slots=mem_slots, \n",
    "                                      head_size=head_size)\n",
    "        self.initial_memory = self._core.initial_state(batch_size=batch_size)\n",
    "        \n",
    "        final_mlp_modules = nn.ModuleList(\n",
    "            [nn.Sequential(\n",
    "                nn.Linear(in_features=self._core._mem_size * self._core._mem_slots,\n",
    "                          out_features=mlp_size),\n",
    "                nn.ReLU())] +\n",
    "            [nn.Sequential(\n",
    "                nn.Linear(in_features=mlp_size,\n",
    "                          out_features=mlp_size),\n",
    "                nn.ReLU())] * (mlp_layers - 2) +\n",
    "            [nn.Linear(in_features=mlp_size,\n",
    "                       out_features=mlp_size)]\n",
    "        )\n",
    "        self._final_mlp = nn.Sequential(*final_mlp_modules)\n",
    "        \n",
    "        self._linear = nn.Linear(in_features=mlp_size,\n",
    "                                 out_features=num_objects)\n",
    "        \n",
    "    # inputs: [B, T, F]\n",
    "    def forward(self, inputs, memory):\n",
    "        output_sequence, output_memory = self._core(inputs, memory)\n",
    "        outputs = output_sequence[:, -1, :].unsqueeze(1)\n",
    "        \n",
    "        outputs = self._final_mlp(outputs)\n",
    "        logits = self._linear(outputs)\n",
    "        \n",
    "        return logits, output_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SequenceModel(input_size=input_size,\n",
    "                      mem_slots=mem_slots, \n",
    "                      head_size=head_size, \n",
    "                      batch_size=batch_size,\n",
    "                      mlp_size=mlp_size, \n",
    "                      mlp_layers=mlp_layers, \n",
    "                      num_objects=num_objects).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6942892074584961\n",
      "loss:  0.6935383081436157\n",
      "loss:  0.6925140619277954\n",
      "loss:  0.6945618987083435\n",
      "loss:  0.6905921101570129\n",
      "loss:  0.6945213675498962\n",
      "loss:  0.6923407912254333\n",
      "loss:  0.6921948790550232\n",
      "loss:  0.6932103633880615\n",
      "loss:  0.694120466709137\n",
      "loss:  0.6937230825424194\n",
      "loss:  0.6945542693138123\n",
      "loss:  0.6945928335189819\n",
      "loss:  0.6946831941604614\n",
      "loss:  0.6922829151153564\n",
      "loss:  0.6932870745658875\n",
      "loss:  0.6935537457466125\n",
      "loss:  0.6908676624298096\n",
      "loss:  0.6940553784370422\n",
      "loss:  0.6896588802337646\n",
      "loss:  0.6898375749588013\n",
      "loss:  0.6892287731170654\n",
      "loss:  0.6900903582572937\n",
      "loss:  0.6927318572998047\n",
      "loss:  0.6960955262184143\n",
      "loss:  0.6928524971008301\n",
      "loss:  0.6903606057167053\n",
      "loss:  0.6910958290100098\n",
      "loss:  0.6878944039344788\n",
      "loss:  0.6993302702903748\n",
      "loss:  0.692302405834198\n",
      "loss:  0.6904089450836182\n",
      "loss:  0.6917802691459656\n",
      "loss:  0.6916359663009644\n",
      "loss:  0.6857774257659912\n",
      "loss:  0.6941584944725037\n",
      "loss:  0.6943195462226868\n",
      "loss:  0.6914940476417542\n",
      "loss:  0.685297966003418\n",
      "loss:  0.6952797770500183\n",
      "loss:  0.684518039226532\n",
      "loss:  0.6946765780448914\n",
      "loss:  0.689264178276062\n",
      "loss:  0.6878076195716858\n",
      "loss:  0.6787930727005005\n",
      "loss:  0.6906107068061829\n",
      "loss:  0.6955870985984802\n",
      "loss:  0.6980692148208618\n",
      "loss:  0.694623589515686\n",
      "loss:  0.6954295635223389\n",
      "loss:  0.6913859844207764\n",
      "loss:  0.6993821859359741\n",
      "loss:  0.6962072849273682\n",
      "loss:  0.693066418170929\n",
      "loss:  0.6881574988365173\n",
      "loss:  0.6900594830513\n",
      "loss:  0.7000350952148438\n",
      "loss:  0.6977900266647339\n",
      "loss:  0.6952424645423889\n",
      "loss:  0.7029715180397034\n",
      "loss:  0.6966294050216675\n",
      "loss:  0.6926655769348145\n",
      "loss:  0.6910585761070251\n",
      "loss:  0.6827772855758667\n",
      "loss:  0.6929973363876343\n",
      "loss:  0.6927957534790039\n",
      "loss:  0.7005289793014526\n",
      "loss:  0.6838883757591248\n",
      "loss:  0.6888009905815125\n",
      "loss:  0.6988363862037659\n",
      "loss:  0.7023791670799255\n",
      "loss:  0.6818935871124268\n",
      "loss:  0.6916676163673401\n",
      "loss:  0.6956967711448669\n",
      "loss:  0.6994406580924988\n",
      "loss:  0.6945118308067322\n",
      "loss:  0.6994335651397705\n",
      "loss:  0.6983546018600464\n",
      "loss:  0.6943989992141724\n",
      "loss:  0.6998820304870605\n",
      "loss:  0.6856135129928589\n",
      "loss:  0.6883859038352966\n",
      "loss:  0.6907017230987549\n",
      "loss:  0.6937024593353271\n",
      "loss:  0.6863946318626404\n",
      "loss:  0.6991716623306274\n",
      "loss:  0.6935905814170837\n",
      "loss:  0.6984459757804871\n",
      "loss:  0.684872031211853\n",
      "loss:  0.6918116807937622\n",
      "loss:  0.6925689578056335\n",
      "loss:  0.7011737823486328\n",
      "loss:  0.6965000629425049\n",
      "loss:  0.6841694116592407\n",
      "loss:  0.6908338069915771\n",
      "loss:  0.6899987459182739\n",
      "loss:  0.6960806250572205\n",
      "loss:  0.6943694353103638\n",
      "loss:  0.6968425512313843\n",
      "loss:  0.6946398019790649\n",
      "loss:  0.6926161050796509\n",
      "loss:  0.6963845491409302\n",
      "loss:  0.6906266212463379\n",
      "loss:  0.6988903284072876\n",
      "loss:  0.7002413272857666\n",
      "loss:  0.6896106004714966\n",
      "loss:  0.6909450888633728\n",
      "loss:  0.6870004534721375\n",
      "loss:  0.6858096718788147\n",
      "loss:  0.6956820487976074\n",
      "loss:  0.6887563467025757\n",
      "loss:  0.6936329007148743\n",
      "loss:  0.6915791034698486\n",
      "loss:  0.6997728943824768\n",
      "loss:  0.6962034106254578\n",
      "loss:  0.689812183380127\n",
      "loss:  0.6881182193756104\n",
      "loss:  0.6882919073104858\n",
      "loss:  0.687488853931427\n",
      "loss:  0.6872388124465942\n",
      "loss:  0.6934740543365479\n",
      "loss:  0.6963085532188416\n",
      "loss:  0.6860761046409607\n",
      "loss:  0.6876971125602722\n",
      "loss:  0.6867179274559021\n",
      "loss:  0.6868669986724854\n",
      "loss:  0.6848918199539185\n",
      "loss:  0.6947712898254395\n",
      "loss:  0.6968901753425598\n",
      "loss:  0.6940422058105469\n",
      "loss:  0.6956371665000916\n",
      "loss:  0.6977635622024536\n",
      "loss:  0.6960676312446594\n",
      "loss:  0.6827753186225891\n",
      "loss:  0.693684458732605\n",
      "loss:  0.686860203742981\n",
      "loss:  0.6972126960754395\n",
      "loss:  0.696772038936615\n",
      "loss:  0.6983727216720581\n",
      "loss:  0.6870300769805908\n",
      "loss:  0.6945594549179077\n",
      "loss:  0.7003123760223389\n",
      "loss:  0.6941906213760376\n",
      "loss:  0.6914758682250977\n",
      "loss:  0.6959114074707031\n",
      "loss:  0.6905488967895508\n",
      "loss:  0.6856527328491211\n",
      "loss:  0.699461817741394\n",
      "loss:  0.6941924095153809\n",
      "loss:  0.6955787539482117\n",
      "loss:  0.6960046291351318\n",
      "loss:  0.6997734308242798\n",
      "loss:  0.6925012469291687\n",
      "loss:  0.6938974857330322\n",
      "loss:  0.7026238441467285\n",
      "loss:  0.6908608675003052\n",
      "loss:  0.6900199055671692\n",
      "loss:  0.691474974155426\n",
      "loss:  0.7027143836021423\n",
      "loss:  0.6985665559768677\n",
      "loss:  0.6992970705032349\n",
      "loss:  0.7018557190895081\n",
      "loss:  0.7021514773368835\n",
      "loss:  0.6887891292572021\n",
      "loss:  0.6952321529388428\n",
      "loss:  0.6868479251861572\n",
      "loss:  0.6891911625862122\n",
      "loss:  0.6898635029792786\n",
      "loss:  0.7012655735015869\n",
      "loss:  0.692479133605957\n",
      "loss:  0.6928992867469788\n",
      "loss:  0.6984520554542542\n",
      "loss:  0.6882026791572571\n",
      "loss:  0.6935014724731445\n",
      "loss:  0.6900291442871094\n",
      "loss:  0.695283055305481\n",
      "loss:  0.6910752654075623\n",
      "loss:  0.6870496869087219\n",
      "loss:  0.6981818079948425\n",
      "loss:  0.6843029260635376\n",
      "loss:  0.6892760992050171\n",
      "loss:  0.6883537173271179\n",
      "loss:  0.6901456713676453\n",
      "loss:  0.6948573589324951\n",
      "loss:  0.6983404159545898\n",
      "loss:  0.6881482601165771\n",
      "loss:  0.6993741393089294\n",
      "loss:  0.699400007724762\n",
      "loss:  0.701992392539978\n",
      "loss:  0.6878753900527954\n",
      "loss:  0.6890769004821777\n",
      "loss:  0.6902446150779724\n",
      "loss:  0.6936322450637817\n",
      "loss:  0.6859620809555054\n",
      "loss:  0.6901057958602905\n",
      "loss:  0.6938074231147766\n",
      "loss:  0.6928929686546326\n",
      "loss:  0.6931778788566589\n",
      "loss:  0.698808491230011\n",
      "loss:  0.6843848824501038\n",
      "loss:  0.6984650492668152\n",
      "loss:  0.6999604105949402\n",
      "loss:  0.7021045684814453\n",
      "loss:  0.6920490860939026\n",
      "loss:  0.687234103679657\n",
      "loss:  0.6958487629890442\n",
      "loss:  0.6906916499137878\n",
      "loss:  0.6898223161697388\n",
      "loss:  0.6948751211166382\n",
      "loss:  0.696696937084198\n",
      "loss:  0.694746196269989\n",
      "loss:  0.6983478665351868\n",
      "loss:  0.6905479431152344\n",
      "loss:  0.6929144263267517\n",
      "loss:  0.6888459920883179\n",
      "loss:  0.6881613731384277\n",
      "loss:  0.693310558795929\n",
      "loss:  0.6915934681892395\n",
      "loss:  0.6980582475662231\n",
      "loss:  0.695240318775177\n",
      "loss:  0.6898247003555298\n",
      "loss:  0.6932591199874878\n",
      "loss:  0.6954328417778015\n",
      "loss:  0.6911316514015198\n",
      "loss:  0.6938920021057129\n",
      "loss:  0.6930480599403381\n",
      "loss:  0.6926256418228149\n",
      "loss:  0.6965468525886536\n",
      "loss:  0.6958015561103821\n",
      "loss:  0.6902561783790588\n",
      "loss:  0.6981280446052551\n",
      "loss:  0.6893210411071777\n",
      "loss:  0.6946422457695007\n",
      "loss:  0.6947744488716125\n",
      "loss:  0.6910082101821899\n",
      "loss:  0.6894579529762268\n",
      "loss:  0.6946595907211304\n",
      "loss:  0.6921111345291138\n",
      "loss:  0.6893362998962402\n",
      "loss:  0.6912939548492432\n",
      "loss:  0.6950812339782715\n",
      "loss:  0.6856325268745422\n",
      "loss:  0.6990322470664978\n",
      "loss:  0.6951841115951538\n",
      "loss:  0.6928114891052246\n",
      "loss:  0.6953750252723694\n",
      "loss:  0.690858006477356\n",
      "loss:  0.6947285532951355\n",
      "loss:  0.6930341720581055\n",
      "loss:  0.6917407512664795\n",
      "loss:  0.6950405240058899\n",
      "loss:  0.6928996443748474\n",
      "loss:  0.6903378367424011\n",
      "loss:  0.6886810660362244\n",
      "loss:  0.6912838816642761\n",
      "loss:  0.6919073462486267\n",
      "loss:  0.6936826705932617\n",
      "loss:  0.6965731978416443\n",
      "loss:  0.6840947866439819\n",
      "loss:  0.6957692503929138\n",
      "loss:  0.6934795379638672\n",
      "loss:  0.6868062019348145\n",
      "loss:  0.6888459324836731\n",
      "loss:  0.6974709033966064\n",
      "loss:  0.6985008120536804\n",
      "loss:  0.693631112575531\n",
      "loss:  0.6952375769615173\n",
      "loss:  0.6928137540817261\n",
      "loss:  0.6926516890525818\n",
      "loss:  0.694500744342804\n",
      "loss:  0.692819356918335\n",
      "loss:  0.6894909143447876\n",
      "loss:  0.6891932487487793\n",
      "loss:  0.6926771402359009\n",
      "loss:  0.692399799823761\n",
      "loss:  0.6949758529663086\n",
      "loss:  0.7006627917289734\n",
      "loss:  0.6899695992469788\n",
      "loss:  0.6887726783752441\n",
      "loss:  0.6873824596405029\n",
      "loss:  0.6931889057159424\n",
      "loss:  0.7053326964378357\n",
      "loss:  0.6925177574157715\n",
      "loss:  0.6884016394615173\n",
      "loss:  0.6879475116729736\n",
      "loss:  0.6966760158538818\n",
      "loss:  0.7011504173278809\n",
      "loss:  0.6872063875198364\n",
      "loss:  0.6944639682769775\n",
      "loss:  0.6940488219261169\n",
      "loss:  0.6932863593101501\n",
      "loss:  0.691214919090271\n",
      "loss:  0.6904165744781494\n",
      "loss:  0.6973568201065063\n",
      "loss:  0.6910788416862488\n",
      "loss:  0.6933674216270447\n",
      "loss:  0.7038309574127197\n",
      "loss:  0.6935667395591736\n",
      "loss:  0.6977856755256653\n",
      "loss:  0.6936246752738953\n",
      "loss:  0.6960662007331848\n",
      "loss:  0.6884284615516663\n",
      "loss:  0.6948249936103821\n",
      "loss:  0.6925289630889893\n",
      "loss:  0.697440505027771\n",
      "loss:  0.6960816383361816\n",
      "loss:  0.6911813616752625\n",
      "loss:  0.6951408386230469\n",
      "loss:  0.6954326033592224\n",
      "loss:  0.6930830478668213\n",
      "loss:  0.6920409798622131\n",
      "loss:  0.6955199241638184\n",
      "loss:  0.685827910900116\n",
      "loss:  0.6957837343215942\n",
      "loss:  0.696208655834198\n",
      "loss:  0.6926630735397339\n",
      "loss:  0.691973090171814\n",
      "loss:  0.6956074237823486\n",
      "loss:  0.7000040411949158\n",
      "loss:  0.6970442533493042\n",
      "loss:  0.6934891939163208\n",
      "loss:  0.7044268846511841\n",
      "loss:  0.6925147771835327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6912351250648499\n",
      "loss:  0.689175009727478\n",
      "loss:  0.698544979095459\n",
      "loss:  0.6893028616905212\n",
      "loss:  0.6962412595748901\n",
      "loss:  0.6938909888267517\n",
      "loss:  0.6864912509918213\n",
      "loss:  0.6960169076919556\n",
      "loss:  0.6963045597076416\n",
      "loss:  0.6964896321296692\n",
      "loss:  0.6982907652854919\n",
      "loss:  0.6929759383201599\n",
      "loss:  0.6953871250152588\n",
      "loss:  0.6995837092399597\n",
      "loss:  0.6931304335594177\n",
      "loss:  0.6932345032691956\n",
      "loss:  0.6984139680862427\n",
      "loss:  0.6922567486763\n",
      "loss:  0.6921203136444092\n",
      "loss:  0.6946203708648682\n",
      "loss:  0.6945300102233887\n",
      "loss:  0.7013130187988281\n",
      "loss:  0.6869336366653442\n",
      "loss:  0.6879980564117432\n",
      "loss:  0.6965294480323792\n",
      "loss:  0.6940479278564453\n",
      "loss:  0.6900445222854614\n",
      "loss:  0.6925024390220642\n",
      "loss:  0.6875962018966675\n",
      "loss:  0.6960646510124207\n",
      "loss:  0.6960728168487549\n",
      "loss:  0.697056770324707\n",
      "loss:  0.6928101181983948\n",
      "loss:  0.69049471616745\n",
      "loss:  0.6898995041847229\n",
      "loss:  0.6950774192810059\n",
      "loss:  0.6837461590766907\n",
      "loss:  0.6899386644363403\n",
      "loss:  0.6956159472465515\n",
      "loss:  0.6933732032775879\n",
      "loss:  0.6982806921005249\n",
      "loss:  0.6925578117370605\n",
      "loss:  0.687677264213562\n",
      "loss:  0.7015516757965088\n",
      "loss:  0.6854360699653625\n",
      "loss:  0.6948115229606628\n",
      "loss:  0.6940538287162781\n",
      "loss:  0.692920982837677\n",
      "loss:  0.690233588218689\n",
      "loss:  0.6933270692825317\n",
      "loss:  0.6864223480224609\n",
      "loss:  0.6934986710548401\n",
      "loss:  0.6956878900527954\n",
      "loss:  0.6979141235351562\n",
      "loss:  0.6907860040664673\n",
      "loss:  0.692987859249115\n",
      "loss:  0.6977202296257019\n",
      "loss:  0.6971006989479065\n",
      "loss:  0.6928583979606628\n",
      "loss:  0.6896902918815613\n",
      "loss:  0.6972770690917969\n",
      "loss:  0.685408890247345\n",
      "loss:  0.6936349868774414\n",
      "loss:  0.6858225464820862\n",
      "loss:  0.7014321088790894\n",
      "loss:  0.6970021724700928\n",
      "loss:  0.6887639760971069\n",
      "loss:  0.7006590962409973\n",
      "loss:  0.7002837061882019\n",
      "loss:  0.694946825504303\n",
      "loss:  0.6956099271774292\n",
      "loss:  0.6863224506378174\n",
      "loss:  0.6995178461074829\n",
      "loss:  0.6942965388298035\n",
      "loss:  0.6975900530815125\n",
      "loss:  0.686660647392273\n",
      "loss:  0.6893989443778992\n",
      "loss:  0.6893733143806458\n",
      "loss:  0.6892424821853638\n",
      "loss:  0.6953116059303284\n",
      "loss:  0.6870726346969604\n",
      "loss:  0.7008838653564453\n",
      "loss:  0.6916511654853821\n",
      "loss:  0.6930396556854248\n",
      "loss:  0.7006772756576538\n",
      "loss:  0.687719464302063\n",
      "loss:  0.6882120370864868\n",
      "loss:  0.6947658658027649\n",
      "loss:  0.694074273109436\n",
      "loss:  0.6970599293708801\n",
      "loss:  0.6955323815345764\n",
      "loss:  0.6963709592819214\n",
      "loss:  0.6967319250106812\n",
      "loss:  0.6939784288406372\n",
      "loss:  0.69297194480896\n",
      "loss:  0.6959952116012573\n",
      "loss:  0.6966567635536194\n",
      "loss:  0.6863400936126709\n",
      "loss:  0.6897080540657043\n",
      "loss:  0.6935973763465881\n",
      "loss:  0.7000525593757629\n",
      "loss:  0.6993544697761536\n",
      "loss:  0.7028846740722656\n",
      "loss:  0.6877114176750183\n",
      "loss:  0.7078179121017456\n",
      "loss:  0.6936123371124268\n",
      "loss:  0.6896362900733948\n",
      "loss:  0.6961525678634644\n",
      "loss:  0.6938753128051758\n",
      "loss:  0.6872335076332092\n",
      "loss:  0.6928197741508484\n",
      "loss:  0.6945478916168213\n",
      "loss:  0.6932926177978516\n",
      "loss:  0.689047634601593\n",
      "loss:  0.6938616633415222\n",
      "loss:  0.6944388747215271\n",
      "loss:  0.6891041398048401\n",
      "loss:  0.7005670070648193\n",
      "loss:  0.6871556043624878\n",
      "loss:  0.6852887868881226\n",
      "loss:  0.6940699815750122\n",
      "loss:  0.6976413726806641\n",
      "loss:  0.6921098828315735\n",
      "loss:  0.6872550249099731\n",
      "loss:  0.6896200776100159\n",
      "loss:  0.6983324289321899\n",
      "loss:  0.6945986151695251\n",
      "loss:  0.6940595507621765\n",
      "loss:  0.7001891732215881\n",
      "loss:  0.6916295289993286\n",
      "loss:  0.6917083859443665\n",
      "loss:  0.6957066059112549\n",
      "loss:  0.6842237710952759\n",
      "loss:  0.6898141503334045\n",
      "loss:  0.694358229637146\n",
      "loss:  0.68967604637146\n",
      "loss:  0.6959860920906067\n",
      "loss:  0.6942319869995117\n",
      "loss:  0.6938085556030273\n",
      "loss:  0.6965328454971313\n",
      "loss:  0.6932095885276794\n",
      "loss:  0.6929742097854614\n",
      "loss:  0.6859710812568665\n",
      "loss:  0.7015601992607117\n",
      "loss:  0.6851475238800049\n",
      "loss:  0.6913874745368958\n",
      "loss:  0.6887661218643188\n",
      "loss:  0.6953785419464111\n",
      "loss:  0.6940622329711914\n",
      "loss:  0.6873835921287537\n",
      "loss:  0.6960986256599426\n",
      "loss:  0.6944860816001892\n",
      "loss:  0.6972812414169312\n",
      "loss:  0.6850801706314087\n",
      "loss:  0.6929528713226318\n",
      "loss:  0.6874815821647644\n",
      "loss:  0.6978485584259033\n",
      "loss:  0.6978528499603271\n",
      "loss:  0.6927424669265747\n",
      "loss:  0.6997900009155273\n",
      "loss:  0.6902284026145935\n",
      "loss:  0.6887742877006531\n",
      "loss:  0.6971833109855652\n",
      "loss:  0.6992193460464478\n",
      "loss:  0.6969428062438965\n",
      "loss:  0.6913450956344604\n",
      "loss:  0.6950420141220093\n",
      "loss:  0.6910874843597412\n",
      "loss:  0.6984516382217407\n",
      "loss:  0.6929329633712769\n",
      "loss:  0.6880176067352295\n",
      "loss:  0.6827793121337891\n",
      "loss:  0.6952159404754639\n",
      "loss:  0.6891958117485046\n",
      "loss:  0.6941453218460083\n",
      "loss:  0.6810942888259888\n",
      "loss:  0.6897651553153992\n",
      "loss:  0.6961058378219604\n",
      "loss:  0.6886332035064697\n",
      "loss:  0.6931328773498535\n",
      "loss:  0.7022140622138977\n",
      "loss:  0.6933718919754028\n",
      "loss:  0.7027970552444458\n",
      "loss:  0.6869819164276123\n",
      "loss:  0.6836822628974915\n",
      "loss:  0.6843338012695312\n",
      "loss:  0.6912261247634888\n",
      "loss:  0.6858899593353271\n",
      "loss:  0.6979334950447083\n",
      "loss:  0.686737060546875\n",
      "loss:  0.6910424828529358\n",
      "loss:  0.6948754191398621\n",
      "loss:  0.689479410648346\n",
      "loss:  0.7003082036972046\n",
      "loss:  0.6968170404434204\n",
      "loss:  0.7020042538642883\n",
      "loss:  0.6899106502532959\n",
      "loss:  0.6934100389480591\n",
      "loss:  0.6862332820892334\n",
      "loss:  0.7033668756484985\n",
      "loss:  0.6962504386901855\n",
      "loss:  0.6843137145042419\n",
      "loss:  0.6893666982650757\n",
      "loss:  0.686448335647583\n",
      "loss:  0.697911262512207\n",
      "loss:  0.6987325549125671\n",
      "loss:  0.6966723203659058\n",
      "loss:  0.6895371079444885\n",
      "loss:  0.6931517720222473\n",
      "loss:  0.6896839141845703\n",
      "loss:  0.6937760710716248\n",
      "loss:  0.6933805346488953\n",
      "loss:  0.6887645721435547\n",
      "loss:  0.6878103017807007\n",
      "loss:  0.6940393447875977\n",
      "loss:  0.682094156742096\n",
      "loss:  0.6995556354522705\n",
      "loss:  0.7002493739128113\n",
      "loss:  0.6925227642059326\n",
      "loss:  0.6939852833747864\n",
      "loss:  0.6939406991004944\n",
      "loss:  0.6931095123291016\n",
      "loss:  0.6843461394309998\n",
      "loss:  0.6906033158302307\n",
      "loss:  0.6975364685058594\n",
      "loss:  0.6983819603919983\n",
      "loss:  0.6968542337417603\n",
      "loss:  0.6977888345718384\n",
      "loss:  0.702751636505127\n",
      "loss:  0.6890746355056763\n",
      "loss:  0.6929081678390503\n",
      "loss:  0.688663899898529\n",
      "loss:  0.690414309501648\n",
      "loss:  0.7025967836380005\n",
      "loss:  0.7017168998718262\n",
      "loss:  0.686851978302002\n",
      "loss:  0.6914207935333252\n",
      "loss:  0.6953169703483582\n",
      "loss:  0.6898830533027649\n",
      "loss:  0.6980191469192505\n",
      "loss:  0.687463104724884\n",
      "loss:  0.6979153752326965\n",
      "loss:  0.6927396059036255\n",
      "loss:  0.6911851763725281\n",
      "loss:  0.7025502324104309\n",
      "loss:  0.6978042721748352\n",
      "loss:  0.682730495929718\n",
      "loss:  0.6870569586753845\n",
      "loss:  0.691646158695221\n",
      "loss:  0.6900703310966492\n",
      "loss:  0.7035925984382629\n",
      "loss:  0.6917130351066589\n",
      "loss:  0.6927845478057861\n",
      "loss:  0.6990159749984741\n",
      "loss:  0.6892484426498413\n",
      "loss:  0.6924278140068054\n",
      "loss:  0.6959953308105469\n",
      "loss:  0.69387286901474\n",
      "loss:  0.6922959685325623\n",
      "loss:  0.6947183012962341\n",
      "loss:  0.6813562512397766\n",
      "loss:  0.6890587210655212\n",
      "loss:  0.7008358240127563\n",
      "loss:  0.6957595944404602\n",
      "loss:  0.6966175436973572\n",
      "loss:  0.6988883018493652\n",
      "loss:  0.6862066388130188\n",
      "loss:  0.6947973370552063\n",
      "loss:  0.6948069930076599\n",
      "loss:  0.6941490769386292\n",
      "loss:  0.6997765302658081\n",
      "loss:  0.6866973042488098\n",
      "loss:  0.6934525966644287\n",
      "loss:  0.6999900937080383\n",
      "loss:  0.6987816691398621\n",
      "loss:  0.6957399845123291\n",
      "loss:  0.6917849779129028\n",
      "loss:  0.6959555149078369\n",
      "loss:  0.6899603009223938\n",
      "loss:  0.6952927708625793\n",
      "loss:  0.689517617225647\n",
      "loss:  0.6910421252250671\n",
      "loss:  0.6931631565093994\n",
      "loss:  0.697334349155426\n",
      "loss:  0.6939584016799927\n",
      "loss:  0.6863710284233093\n",
      "loss:  0.6946069598197937\n",
      "loss:  0.6968511343002319\n",
      "loss:  0.692596971988678\n",
      "loss:  0.6935189366340637\n",
      "loss:  0.694701075553894\n",
      "loss:  0.6861391663551331\n",
      "loss:  0.6820369362831116\n",
      "loss:  0.6933862566947937\n",
      "loss:  0.6947524547576904\n",
      "loss:  0.6921101808547974\n",
      "loss:  0.692267656326294\n",
      "loss:  0.691608726978302\n",
      "loss:  0.6919838786125183\n",
      "loss:  0.7008649706840515\n",
      "loss:  0.6988012194633484\n",
      "loss:  0.6911696791648865\n",
      "loss:  0.6915603280067444\n",
      "loss:  0.6977089047431946\n",
      "loss:  0.6980965733528137\n",
      "loss:  0.6949805021286011\n",
      "loss:  0.6996782422065735\n",
      "loss:  0.6862339973449707\n",
      "loss:  0.6949271559715271\n",
      "loss:  0.6896313428878784\n",
      "loss:  0.6897372603416443\n",
      "loss:  0.697172224521637\n",
      "loss:  0.692688524723053\n",
      "loss:  0.6857180595397949\n",
      "loss:  0.6851394176483154\n",
      "loss:  0.6909674406051636\n",
      "loss:  0.6931739449501038\n",
      "loss:  0.6995460987091064\n",
      "loss:  0.6961925029754639\n",
      "loss:  0.6928887963294983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6887850165367126\n",
      "loss:  0.6933727264404297\n",
      "loss:  0.6965416073799133\n",
      "loss:  0.6918244957923889\n",
      "loss:  0.6885743737220764\n",
      "loss:  0.6967795491218567\n",
      "loss:  0.6963155269622803\n",
      "loss:  0.6893298625946045\n",
      "loss:  0.684684157371521\n",
      "loss:  0.693168580532074\n",
      "loss:  0.6973685026168823\n",
      "loss:  0.6848653554916382\n",
      "loss:  0.699556291103363\n",
      "loss:  0.6861875057220459\n",
      "loss:  0.6926037669181824\n",
      "loss:  0.6986609101295471\n",
      "loss:  0.70079505443573\n",
      "loss:  0.7001266479492188\n",
      "loss:  0.6875305771827698\n",
      "loss:  0.6969872713088989\n",
      "loss:  0.6914628744125366\n",
      "loss:  0.6970230340957642\n",
      "loss:  0.6944520473480225\n",
      "loss:  0.6938812732696533\n",
      "loss:  0.6973568797111511\n",
      "loss:  0.6905354261398315\n",
      "loss:  0.6966820955276489\n",
      "loss:  0.6878684759140015\n",
      "loss:  0.6967214345932007\n",
      "loss:  0.6850898861885071\n",
      "loss:  0.6861116886138916\n",
      "loss:  0.6928758025169373\n",
      "loss:  0.6992956399917603\n",
      "loss:  0.6935221552848816\n",
      "loss:  0.6884850859642029\n",
      "loss:  0.6931564807891846\n",
      "loss:  0.693383514881134\n",
      "loss:  0.6945047974586487\n",
      "loss:  0.6991778612136841\n",
      "loss:  0.6996936798095703\n",
      "loss:  0.6872623562812805\n",
      "loss:  0.6879482865333557\n",
      "loss:  0.6926429271697998\n",
      "loss:  0.6972448825836182\n",
      "loss:  0.6949085593223572\n",
      "loss:  0.6868680119514465\n",
      "loss:  0.6926286816596985\n",
      "loss:  0.6912603378295898\n",
      "loss:  0.691624641418457\n",
      "loss:  0.6944973468780518\n",
      "loss:  0.6890236139297485\n",
      "loss:  0.6916477680206299\n",
      "loss:  0.6912912726402283\n",
      "loss:  0.6930282115936279\n",
      "loss:  0.6873433589935303\n",
      "loss:  0.6905375123023987\n",
      "loss:  0.7006434202194214\n",
      "loss:  0.6980155110359192\n",
      "loss:  0.6942663788795471\n",
      "loss:  0.693056583404541\n",
      "loss:  0.6918140053749084\n",
      "loss:  0.6875278949737549\n",
      "loss:  0.6934443712234497\n",
      "loss:  0.696021556854248\n",
      "loss:  0.6941450238227844\n",
      "loss:  0.6913630962371826\n",
      "loss:  0.6893354058265686\n",
      "loss:  0.6938018798828125\n",
      "loss:  0.6868691444396973\n",
      "loss:  0.6952515840530396\n",
      "loss:  0.693802535533905\n",
      "loss:  0.6957051753997803\n",
      "loss:  0.6972248554229736\n",
      "loss:  0.6927769184112549\n",
      "loss:  0.689629077911377\n",
      "loss:  0.6964359879493713\n",
      "loss:  0.6913191676139832\n",
      "loss:  0.691226065158844\n",
      "loss:  0.6926056742668152\n",
      "loss:  0.6959171295166016\n",
      "loss:  0.6950823664665222\n",
      "loss:  0.6909633278846741\n",
      "loss:  0.6941932439804077\n",
      "loss:  0.6976117491722107\n",
      "loss:  0.6902733445167542\n",
      "loss:  0.6923739910125732\n",
      "loss:  0.6911910176277161\n",
      "loss:  0.6940534710884094\n",
      "loss:  0.6901577711105347\n",
      "loss:  0.6947751641273499\n",
      "loss:  0.6844404339790344\n",
      "loss:  0.6954466700553894\n",
      "loss:  0.688984215259552\n",
      "loss:  0.694024920463562\n",
      "loss:  0.6952869892120361\n",
      "loss:  0.6976261138916016\n",
      "loss:  0.6901013255119324\n",
      "loss:  0.6977797746658325\n",
      "loss:  0.6957324743270874\n",
      "loss:  0.6908993124961853\n",
      "loss:  0.6906020641326904\n",
      "loss:  0.6910148859024048\n",
      "loss:  0.7003662586212158\n",
      "loss:  0.69398033618927\n",
      "loss:  0.6908549070358276\n",
      "loss:  0.691524088382721\n",
      "loss:  0.6874897480010986\n",
      "loss:  0.7002642750740051\n",
      "loss:  0.6935188174247742\n",
      "loss:  0.6994377374649048\n",
      "loss:  0.6903637647628784\n",
      "loss:  0.6976513862609863\n",
      "loss:  0.7012119889259338\n",
      "loss:  0.69927978515625\n",
      "loss:  0.6964176893234253\n",
      "loss:  0.6896550059318542\n",
      "loss:  0.6934176683425903\n",
      "loss:  0.6914544105529785\n",
      "loss:  0.6892976760864258\n",
      "loss:  0.6939557194709778\n",
      "loss:  0.6920313835144043\n",
      "loss:  0.6932815313339233\n",
      "loss:  0.6925970315933228\n",
      "loss:  0.6909347772598267\n",
      "loss:  0.6956701278686523\n",
      "loss:  0.6967600584030151\n",
      "loss:  0.6879251003265381\n",
      "loss:  0.6927493810653687\n",
      "loss:  0.6950795650482178\n",
      "loss:  0.689802348613739\n",
      "loss:  0.6908755302429199\n",
      "loss:  0.6958628296852112\n",
      "loss:  0.6990253329277039\n",
      "loss:  0.6906874775886536\n",
      "loss:  0.693551242351532\n",
      "loss:  0.6917444467544556\n",
      "loss:  0.6910625696182251\n",
      "loss:  0.6929113864898682\n",
      "loss:  0.6851650476455688\n",
      "loss:  0.7032421827316284\n",
      "loss:  0.6987687349319458\n",
      "loss:  0.6975467205047607\n",
      "loss:  0.6980157494544983\n",
      "loss:  0.6945589780807495\n",
      "loss:  0.6864534020423889\n",
      "loss:  0.7008539438247681\n",
      "loss:  0.6818425059318542\n",
      "loss:  0.6980534195899963\n",
      "loss:  0.6867952942848206\n",
      "loss:  0.6931818127632141\n",
      "loss:  0.6875507831573486\n",
      "loss:  0.6917511820793152\n",
      "loss:  0.6900249123573303\n",
      "loss:  0.690291166305542\n",
      "loss:  0.6925141215324402\n",
      "loss:  0.6955275535583496\n",
      "loss:  0.6898655295372009\n",
      "loss:  0.6922623515129089\n",
      "loss:  0.6952935457229614\n",
      "loss:  0.6911488175392151\n",
      "loss:  0.6899553537368774\n",
      "loss:  0.6888108253479004\n",
      "loss:  0.6937166452407837\n",
      "loss:  0.6849888563156128\n",
      "loss:  0.696253776550293\n",
      "loss:  0.7012102603912354\n",
      "loss:  0.6907292604446411\n",
      "loss:  0.6986582279205322\n",
      "loss:  0.693307101726532\n",
      "loss:  0.693786084651947\n",
      "loss:  0.6900734901428223\n",
      "loss:  0.6924912333488464\n",
      "loss:  0.6971204280853271\n",
      "loss:  0.6914695501327515\n",
      "loss:  0.6897071003913879\n",
      "loss:  0.6968981623649597\n",
      "loss:  0.6970688104629517\n",
      "loss:  0.6954919099807739\n",
      "loss:  0.6926281452178955\n",
      "loss:  0.6896327137947083\n",
      "loss:  0.6896278858184814\n",
      "loss:  0.695979654788971\n",
      "loss:  0.6868480443954468\n",
      "loss:  0.6860286593437195\n",
      "loss:  0.696625292301178\n",
      "loss:  0.6952768564224243\n",
      "loss:  0.6943322420120239\n",
      "loss:  0.6979759931564331\n",
      "loss:  0.6987992525100708\n",
      "loss:  0.6872795820236206\n",
      "loss:  0.6887915730476379\n",
      "loss:  0.695383608341217\n",
      "loss:  0.6848184466362\n",
      "loss:  0.7001323103904724\n",
      "loss:  0.6954538226127625\n",
      "loss:  0.6951900720596313\n",
      "loss:  0.695218026638031\n",
      "loss:  0.6904389262199402\n",
      "loss:  0.693656325340271\n",
      "loss:  0.6919133067131042\n",
      "loss:  0.6954075694084167\n",
      "loss:  0.7014315128326416\n",
      "loss:  0.7008008360862732\n",
      "loss:  0.6989985704421997\n",
      "loss:  0.6942381262779236\n",
      "loss:  0.6914470195770264\n",
      "loss:  0.6851741075515747\n",
      "loss:  0.6982481479644775\n",
      "loss:  0.6939375996589661\n",
      "loss:  0.6946040987968445\n",
      "loss:  0.6907012462615967\n",
      "loss:  0.6882113814353943\n",
      "loss:  0.6939229965209961\n",
      "loss:  0.6904738545417786\n",
      "loss:  0.6874170899391174\n",
      "loss:  0.6916576027870178\n",
      "loss:  0.6968284249305725\n",
      "loss:  0.6956298351287842\n",
      "loss:  0.6888717412948608\n",
      "loss:  0.6968047618865967\n",
      "loss:  0.6975847482681274\n",
      "loss:  0.6887720227241516\n",
      "loss:  0.689746081829071\n",
      "loss:  0.6902320981025696\n",
      "loss:  0.6900044679641724\n",
      "loss:  0.6967895030975342\n",
      "loss:  0.7061647772789001\n",
      "loss:  0.6994602084159851\n",
      "loss:  0.6976389288902283\n",
      "loss:  0.6927053928375244\n",
      "loss:  0.6892057657241821\n",
      "loss:  0.6890673637390137\n",
      "loss:  0.697778046131134\n",
      "loss:  0.6914367079734802\n",
      "loss:  0.6933597922325134\n",
      "loss:  0.697847843170166\n",
      "loss:  0.6995039582252502\n",
      "loss:  0.6911366581916809\n",
      "loss:  0.6938796043395996\n",
      "loss:  0.6971506476402283\n",
      "loss:  0.6964722871780396\n",
      "loss:  0.68433678150177\n",
      "loss:  0.693215012550354\n",
      "loss:  0.6961534023284912\n",
      "loss:  0.6929349899291992\n",
      "loss:  0.6908972859382629\n",
      "loss:  0.6896528601646423\n",
      "loss:  0.6977316737174988\n",
      "loss:  0.6921703815460205\n",
      "loss:  0.6905532479286194\n",
      "loss:  0.6943285465240479\n",
      "loss:  0.6925399303436279\n",
      "loss:  0.7019369006156921\n",
      "loss:  0.6864202618598938\n",
      "loss:  0.6926456689834595\n",
      "loss:  0.6885061264038086\n",
      "loss:  0.6900004744529724\n",
      "loss:  0.6882014870643616\n",
      "loss:  0.6939622759819031\n",
      "loss:  0.6968770623207092\n",
      "loss:  0.69813072681427\n",
      "loss:  0.6939225196838379\n",
      "loss:  0.7025483846664429\n",
      "loss:  0.6961487531661987\n",
      "loss:  0.6925072073936462\n",
      "loss:  0.6904217004776001\n",
      "loss:  0.6907883882522583\n",
      "loss:  0.6940124034881592\n",
      "loss:  0.6922513246536255\n",
      "loss:  0.689123272895813\n",
      "loss:  0.6923603415489197\n",
      "loss:  0.6896987557411194\n",
      "loss:  0.69455486536026\n",
      "loss:  0.6896612644195557\n",
      "loss:  0.6932265162467957\n",
      "loss:  0.6990991830825806\n",
      "loss:  0.6939883232116699\n",
      "loss:  0.6865012645721436\n",
      "loss:  0.6955277323722839\n",
      "loss:  0.6937589645385742\n",
      "loss:  0.6965590715408325\n",
      "loss:  0.6915770173072815\n",
      "loss:  0.6925016641616821\n",
      "loss:  0.6962153911590576\n",
      "loss:  0.6952872276306152\n",
      "loss:  0.6856513023376465\n",
      "loss:  0.6918017268180847\n",
      "loss:  0.6904202699661255\n",
      "loss:  0.6943240165710449\n",
      "loss:  0.6970557570457458\n",
      "loss:  0.6871442198753357\n",
      "loss:  0.6934815645217896\n",
      "loss:  0.694748044013977\n",
      "loss:  0.6943397521972656\n",
      "loss:  0.6893700361251831\n",
      "loss:  0.6964330673217773\n",
      "loss:  0.691749632358551\n",
      "loss:  0.6926200985908508\n",
      "loss:  0.699657678604126\n",
      "loss:  0.6955194473266602\n",
      "loss:  0.6932636499404907\n",
      "loss:  0.6993076205253601\n",
      "loss:  0.6916571855545044\n",
      "loss:  0.6864244341850281\n",
      "loss:  0.6954477429389954\n",
      "loss:  0.6963698863983154\n",
      "loss:  0.6949858665466309\n",
      "loss:  0.6972065567970276\n",
      "loss:  0.6903683543205261\n",
      "loss:  0.6964918971061707\n",
      "loss:  0.692799985408783\n",
      "loss:  0.6926732659339905\n",
      "loss:  0.6936662793159485\n",
      "loss:  0.6860125064849854\n",
      "loss:  0.6978830099105835\n",
      "loss:  0.6927024126052856\n",
      "loss:  0.7007894515991211\n",
      "loss:  0.6953150629997253\n",
      "loss:  0.6913481950759888\n",
      "loss:  0.6917198300361633\n",
      "loss:  0.6931514739990234\n",
      "loss:  0.6951838135719299\n",
      "loss:  0.6901435256004333\n",
      "loss:  0.6967716813087463\n",
      "loss:  0.6929722428321838\n",
      "loss:  0.6954156756401062\n",
      "loss:  0.6951280236244202\n",
      "loss:  0.6925716400146484\n",
      "loss:  0.6912294626235962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6995431184768677\n",
      "loss:  0.6934179663658142\n",
      "loss:  0.6922742128372192\n",
      "loss:  0.692824125289917\n",
      "loss:  0.6947125196456909\n",
      "loss:  0.695656418800354\n",
      "loss:  0.6921481490135193\n",
      "loss:  0.6966071128845215\n",
      "loss:  0.6956300139427185\n",
      "loss:  0.6927282810211182\n",
      "loss:  0.6858585476875305\n",
      "loss:  0.6935818195343018\n",
      "loss:  0.6900894641876221\n",
      "loss:  0.6900123357772827\n",
      "loss:  0.6976837515830994\n",
      "loss:  0.6886906623840332\n",
      "loss:  0.6959361433982849\n",
      "loss:  0.6873558163642883\n",
      "loss:  0.6860365271568298\n",
      "loss:  0.6904100179672241\n",
      "loss:  0.6951452493667603\n",
      "loss:  0.6909766793251038\n",
      "loss:  0.6932520866394043\n",
      "loss:  0.6924836039543152\n",
      "loss:  0.6991391181945801\n",
      "loss:  0.6923537254333496\n",
      "loss:  0.6963680982589722\n",
      "loss:  0.6929829120635986\n",
      "loss:  0.690061628818512\n",
      "loss:  0.6930991411209106\n",
      "loss:  0.6935222148895264\n",
      "loss:  0.6995489597320557\n",
      "loss:  0.6941311955451965\n",
      "loss:  0.6933112740516663\n",
      "loss:  0.6943793892860413\n",
      "loss:  0.6911531090736389\n",
      "loss:  0.6919308304786682\n",
      "loss:  0.6899223923683167\n",
      "loss:  0.690049409866333\n",
      "loss:  0.6913443207740784\n",
      "loss:  0.6911547183990479\n",
      "loss:  0.6973311305046082\n",
      "loss:  0.6900500655174255\n",
      "loss:  0.6905910968780518\n",
      "loss:  0.6911079287528992\n",
      "loss:  0.6892112493515015\n",
      "loss:  0.6933863759040833\n",
      "loss:  0.687811553478241\n",
      "loss:  0.6949716210365295\n",
      "loss:  0.691982626914978\n",
      "loss:  0.6910127997398376\n",
      "loss:  0.688892126083374\n",
      "loss:  0.6901111006736755\n",
      "loss:  0.6928964853286743\n",
      "loss:  0.6987971663475037\n",
      "loss:  0.6892820000648499\n",
      "loss:  0.6935106515884399\n",
      "loss:  0.6961163282394409\n",
      "loss:  0.6915586590766907\n",
      "loss:  0.6951671838760376\n",
      "loss:  0.6989948749542236\n",
      "loss:  0.6869571208953857\n",
      "loss:  0.6921254396438599\n",
      "loss:  0.6948888301849365\n",
      "loss:  0.6896169185638428\n",
      "loss:  0.6848180294036865\n",
      "loss:  0.6962169408798218\n",
      "loss:  0.6985996961593628\n",
      "loss:  0.7016065716743469\n",
      "loss:  0.698617696762085\n",
      "loss:  0.6900851130485535\n",
      "loss:  0.6920582056045532\n",
      "loss:  0.6949658393859863\n",
      "loss:  0.6949334144592285\n",
      "loss:  0.6927258968353271\n",
      "loss:  0.6907252073287964\n",
      "loss:  0.6905505061149597\n",
      "loss:  0.6955553889274597\n",
      "loss:  0.6963610053062439\n",
      "loss:  0.6971973776817322\n",
      "loss:  0.6892067790031433\n",
      "loss:  0.6914819478988647\n",
      "loss:  0.6939443945884705\n",
      "loss:  0.6863069534301758\n",
      "loss:  0.6968063712120056\n",
      "loss:  0.700065016746521\n",
      "loss:  0.6911830306053162\n",
      "loss:  0.6971132159233093\n",
      "loss:  0.6931877732276917\n",
      "loss:  0.6918470859527588\n",
      "loss:  0.6921359896659851\n",
      "loss:  0.6885814666748047\n",
      "loss:  0.6890428066253662\n",
      "loss:  0.6907768249511719\n",
      "loss:  0.6947928071022034\n",
      "loss:  0.6884188652038574\n",
      "loss:  0.6911912560462952\n",
      "loss:  0.6854362487792969\n",
      "loss:  0.6947224140167236\n",
      "loss:  0.6925536394119263\n",
      "loss:  0.6966415047645569\n",
      "loss:  0.6964092254638672\n",
      "loss:  0.6947984099388123\n",
      "loss:  0.6952385306358337\n",
      "loss:  0.6946055889129639\n",
      "loss:  0.6906532049179077\n",
      "loss:  0.7026114463806152\n",
      "loss:  0.700637936592102\n",
      "loss:  0.6890007257461548\n",
      "loss:  0.6970349550247192\n",
      "loss:  0.695682168006897\n",
      "loss:  0.6901759505271912\n",
      "loss:  0.6948382258415222\n",
      "loss:  0.6922217607498169\n",
      "loss:  0.6994786262512207\n",
      "loss:  0.6966399550437927\n",
      "loss:  0.6953790187835693\n",
      "loss:  0.6947556138038635\n",
      "loss:  0.6949861645698547\n",
      "loss:  0.6899597644805908\n",
      "loss:  0.6912283897399902\n",
      "loss:  0.6931765675544739\n",
      "loss:  0.6957468390464783\n",
      "loss:  0.6901508569717407\n",
      "loss:  0.6980319619178772\n",
      "loss:  0.6915978789329529\n",
      "loss:  0.6981936097145081\n",
      "loss:  0.6942797303199768\n",
      "loss:  0.6903022527694702\n",
      "loss:  0.6927341222763062\n",
      "loss:  0.6873540282249451\n",
      "loss:  0.6849830746650696\n",
      "loss:  0.687699556350708\n",
      "loss:  0.6886038184165955\n",
      "loss:  0.6891000866889954\n",
      "loss:  0.6978449821472168\n",
      "loss:  0.6935492157936096\n",
      "loss:  0.6961206793785095\n",
      "loss:  0.6929784417152405\n",
      "loss:  0.6948628425598145\n",
      "loss:  0.6947378516197205\n",
      "loss:  0.6875109672546387\n",
      "loss:  0.6962460875511169\n",
      "loss:  0.6954442262649536\n",
      "loss:  0.6917275786399841\n",
      "loss:  0.7025675177574158\n",
      "loss:  0.6939153075218201\n",
      "loss:  0.6971738338470459\n",
      "loss:  0.6922665238380432\n",
      "loss:  0.6954929232597351\n",
      "loss:  0.691554844379425\n",
      "loss:  0.6995269060134888\n",
      "loss:  0.6994777321815491\n",
      "loss:  0.6937508583068848\n",
      "loss:  0.6927075386047363\n",
      "loss:  0.6944966316223145\n",
      "loss:  0.6876841187477112\n",
      "loss:  0.6964789628982544\n",
      "loss:  0.6905731558799744\n",
      "loss:  0.6905891299247742\n",
      "loss:  0.6889422535896301\n",
      "loss:  0.6958563327789307\n",
      "loss:  0.696365237236023\n",
      "loss:  0.6888599991798401\n",
      "loss:  0.6899169683456421\n",
      "loss:  0.6888347268104553\n",
      "loss:  0.6934513449668884\n",
      "loss:  0.6936081647872925\n",
      "loss:  0.697697103023529\n",
      "loss:  0.6972348093986511\n",
      "loss:  0.6903070211410522\n",
      "loss:  0.6920412182807922\n",
      "loss:  0.6975402235984802\n",
      "loss:  0.6888685822486877\n",
      "loss:  0.694125771522522\n",
      "loss:  0.6951276063919067\n",
      "loss:  0.6930579543113708\n",
      "loss:  0.6880828738212585\n",
      "loss:  0.6958932280540466\n",
      "loss:  0.6894444823265076\n",
      "loss:  0.6955047845840454\n",
      "loss:  0.6929851770401001\n",
      "loss:  0.6985443830490112\n",
      "loss:  0.69246506690979\n",
      "loss:  0.6899009346961975\n",
      "loss:  0.690719723701477\n",
      "loss:  0.6905584335327148\n",
      "loss:  0.6951544284820557\n",
      "loss:  0.6927844882011414\n",
      "loss:  0.6835766434669495\n",
      "loss:  0.6880276799201965\n",
      "loss:  0.6836093068122864\n",
      "loss:  0.6931473016738892\n",
      "loss:  0.6879293918609619\n",
      "loss:  0.6948956251144409\n",
      "loss:  0.688240647315979\n",
      "loss:  0.6927302479743958\n",
      "loss:  0.6973596811294556\n",
      "loss:  0.6893722414970398\n",
      "loss:  0.6869813203811646\n",
      "loss:  0.6810637712478638\n",
      "loss:  0.6903762817382812\n",
      "loss:  0.6914010643959045\n",
      "loss:  0.6972532868385315\n",
      "loss:  0.6920018196105957\n",
      "loss:  0.7012797594070435\n",
      "loss:  0.6907930970191956\n",
      "loss:  0.6924562454223633\n",
      "loss:  0.6972928643226624\n",
      "loss:  0.6940664052963257\n",
      "loss:  0.6906573176383972\n",
      "loss:  0.6906977295875549\n",
      "loss:  0.6896342635154724\n",
      "loss:  0.6989797353744507\n",
      "loss:  0.6970581412315369\n",
      "loss:  0.6892567873001099\n",
      "loss:  0.6966931819915771\n",
      "loss:  0.6886625289916992\n",
      "loss:  0.6967341899871826\n",
      "loss:  0.6930499076843262\n",
      "loss:  0.6868979930877686\n",
      "loss:  0.6940327286720276\n",
      "loss:  0.6884468793869019\n",
      "loss:  0.6916942000389099\n",
      "loss:  0.7003777623176575\n",
      "loss:  0.6902351975440979\n",
      "loss:  0.6880280375480652\n",
      "loss:  0.688931405544281\n",
      "loss:  0.689246654510498\n",
      "loss:  0.6975514888763428\n",
      "loss:  0.6908355951309204\n",
      "loss:  0.6932350397109985\n",
      "loss:  0.6916512250900269\n",
      "loss:  0.6935617923736572\n",
      "loss:  0.6873196959495544\n",
      "loss:  0.6945578455924988\n",
      "loss:  0.6955180764198303\n",
      "loss:  0.6951184272766113\n",
      "loss:  0.6936014294624329\n",
      "loss:  0.6987959146499634\n",
      "loss:  0.6871171593666077\n",
      "loss:  0.6901230812072754\n",
      "loss:  0.6946741342544556\n",
      "loss:  0.6963299512863159\n",
      "loss:  0.6948291659355164\n",
      "loss:  0.6935701966285706\n",
      "loss:  0.6935166716575623\n",
      "loss:  0.6937601566314697\n",
      "loss:  0.697399377822876\n",
      "loss:  0.6918479204177856\n",
      "loss:  0.6956627368927002\n",
      "loss:  0.6949014067649841\n",
      "loss:  0.6887732148170471\n",
      "loss:  0.6914332509040833\n",
      "loss:  0.7001878619194031\n",
      "loss:  0.6968385577201843\n",
      "loss:  0.6837626695632935\n",
      "loss:  0.7012343406677246\n",
      "loss:  0.6877033114433289\n",
      "loss:  0.6934190988540649\n",
      "loss:  0.6849920749664307\n",
      "loss:  0.691061794757843\n",
      "loss:  0.6943984031677246\n",
      "loss:  0.6931346654891968\n",
      "loss:  0.6936381459236145\n",
      "loss:  0.6938867568969727\n",
      "loss:  0.6874495148658752\n",
      "loss:  0.6981319785118103\n",
      "loss:  0.6920943260192871\n",
      "loss:  0.6941664814949036\n",
      "loss:  0.6980891823768616\n",
      "loss:  0.6933350563049316\n",
      "loss:  0.686946451663971\n",
      "loss:  0.6930125951766968\n",
      "loss:  0.6904453635215759\n",
      "loss:  0.6988828182220459\n",
      "loss:  0.6915659308433533\n",
      "loss:  0.6970799565315247\n",
      "loss:  0.6927359700202942\n",
      "loss:  0.6920104026794434\n",
      "loss:  0.6984044313430786\n",
      "loss:  0.6889282464981079\n",
      "loss:  0.6901112794876099\n",
      "loss:  0.6930383443832397\n",
      "loss:  0.6967397332191467\n",
      "loss:  0.693619430065155\n",
      "loss:  0.6870847344398499\n",
      "loss:  0.6927696466445923\n",
      "loss:  0.6963379383087158\n",
      "loss:  0.6859109401702881\n",
      "loss:  0.6976814866065979\n",
      "loss:  0.6960696578025818\n",
      "loss:  0.6844480633735657\n",
      "loss:  0.6942358613014221\n",
      "loss:  0.6952383518218994\n",
      "loss:  0.6952458024024963\n",
      "loss:  0.6921370625495911\n",
      "loss:  0.6941031813621521\n",
      "loss:  0.6906479597091675\n",
      "loss:  0.6928296685218811\n",
      "loss:  0.6909258961677551\n",
      "loss:  0.6886739134788513\n",
      "loss:  0.6962634921073914\n",
      "loss:  0.6840792894363403\n",
      "loss:  0.7003943920135498\n",
      "loss:  0.6917209625244141\n",
      "loss:  0.6954776048660278\n",
      "loss:  0.6940690875053406\n",
      "loss:  0.6890724301338196\n",
      "loss:  0.6889163851737976\n",
      "loss:  0.689543604850769\n",
      "loss:  0.6969203948974609\n",
      "loss:  0.6938337683677673\n",
      "loss:  0.6893662810325623\n",
      "loss:  0.702839732170105\n",
      "loss:  0.6982824802398682\n",
      "loss:  0.6887962818145752\n",
      "loss:  0.6903638243675232\n",
      "loss:  0.6944185495376587\n",
      "loss:  0.6911712884902954\n",
      "loss:  0.7002068758010864\n",
      "loss:  0.6909406781196594\n",
      "loss:  0.692486584186554\n",
      "loss:  0.6985375285148621\n",
      "loss:  0.6927000284194946\n",
      "loss:  0.6958746910095215\n",
      "loss:  0.7016823887825012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6914925575256348\n",
      "loss:  0.6950885653495789\n",
      "loss:  0.6914606690406799\n",
      "loss:  0.6920283436775208\n",
      "loss:  0.6919340491294861\n",
      "loss:  0.6947298049926758\n",
      "loss:  0.698607861995697\n",
      "loss:  0.6943261027336121\n",
      "loss:  0.6917824149131775\n",
      "loss:  0.7019763588905334\n",
      "loss:  0.6904352903366089\n",
      "loss:  0.6865838170051575\n",
      "loss:  0.6930618286132812\n",
      "loss:  0.6960722804069519\n",
      "loss:  0.6984136700630188\n",
      "loss:  0.6922347545623779\n",
      "loss:  0.6981207728385925\n",
      "loss:  0.6998845338821411\n",
      "loss:  0.6925809979438782\n",
      "loss:  0.6943039894104004\n",
      "loss:  0.6938746571540833\n",
      "loss:  0.6935257911682129\n",
      "loss:  0.688777506351471\n",
      "loss:  0.6884879469871521\n",
      "loss:  0.6978108286857605\n",
      "loss:  0.6981193423271179\n",
      "loss:  0.6847246885299683\n",
      "loss:  0.6980490684509277\n",
      "loss:  0.6945961713790894\n",
      "loss:  0.6947721838951111\n",
      "loss:  0.6967014670372009\n",
      "loss:  0.6997168660163879\n",
      "loss:  0.6909884214401245\n",
      "loss:  0.6921544075012207\n",
      "loss:  0.6883059144020081\n",
      "loss:  0.6995817422866821\n",
      "loss:  0.7017131447792053\n",
      "loss:  0.6942703723907471\n",
      "loss:  0.6964253783226013\n",
      "loss:  0.6941809058189392\n",
      "loss:  0.6873738765716553\n",
      "loss:  0.7008581757545471\n",
      "loss:  0.6956139802932739\n",
      "loss:  0.6970168352127075\n",
      "loss:  0.6862856149673462\n",
      "loss:  0.6885483264923096\n",
      "loss:  0.6911445260047913\n",
      "loss:  0.6877841353416443\n",
      "loss:  0.694153904914856\n",
      "loss:  0.7015029191970825\n",
      "loss:  0.687574565410614\n",
      "loss:  0.693878173828125\n",
      "loss:  0.6976171135902405\n",
      "loss:  0.6962550282478333\n",
      "loss:  0.7017228603363037\n",
      "loss:  0.6841865181922913\n",
      "loss:  0.6967896819114685\n",
      "loss:  0.7005694508552551\n",
      "loss:  0.6965763568878174\n",
      "loss:  0.6888485550880432\n",
      "loss:  0.7002685070037842\n",
      "loss:  0.692371129989624\n",
      "loss:  0.6935192942619324\n",
      "loss:  0.695534348487854\n",
      "loss:  0.6906432509422302\n",
      "loss:  0.6954109072685242\n",
      "loss:  0.69399094581604\n",
      "loss:  0.7004147171974182\n",
      "loss:  0.697982907295227\n",
      "loss:  0.6858649253845215\n",
      "loss:  0.6970080137252808\n",
      "loss:  0.695855975151062\n",
      "loss:  0.6984159350395203\n",
      "loss:  0.7016006708145142\n",
      "loss:  0.6894755363464355\n",
      "loss:  0.6856806874275208\n",
      "loss:  0.6896901726722717\n",
      "loss:  0.7004509568214417\n",
      "loss:  0.7030365467071533\n",
      "loss:  0.6848722100257874\n",
      "loss:  0.6924194693565369\n",
      "loss:  0.6942664384841919\n",
      "loss:  0.6968560218811035\n",
      "loss:  0.6922764182090759\n",
      "loss:  0.6921963691711426\n",
      "loss:  0.6884646415710449\n",
      "loss:  0.6975675821304321\n",
      "loss:  0.6929481029510498\n",
      "loss:  0.6927962899208069\n",
      "loss:  0.702538013458252\n",
      "loss:  0.6886376738548279\n",
      "loss:  0.6875530481338501\n",
      "loss:  0.6868008971214294\n",
      "loss:  0.6935200095176697\n",
      "loss:  0.6997967958450317\n",
      "loss:  0.683883011341095\n",
      "loss:  0.6986212730407715\n",
      "loss:  0.6875981688499451\n",
      "loss:  0.6954395174980164\n",
      "loss:  0.6942494511604309\n",
      "loss:  0.6942264437675476\n",
      "loss:  0.6970951557159424\n",
      "loss:  0.6905144453048706\n",
      "loss:  0.6865050792694092\n",
      "loss:  0.6927158236503601\n",
      "loss:  0.6921738982200623\n",
      "loss:  0.6921642422676086\n",
      "loss:  0.6960686445236206\n",
      "loss:  0.6940727829933167\n",
      "loss:  0.6969848871231079\n",
      "loss:  0.6964198350906372\n",
      "loss:  0.6983141303062439\n",
      "loss:  0.6900227665901184\n",
      "loss:  0.6960971355438232\n",
      "loss:  0.6890169382095337\n",
      "loss:  0.6870338916778564\n",
      "loss:  0.6908054947853088\n",
      "loss:  0.6809797286987305\n",
      "loss:  0.7012740969657898\n",
      "loss:  0.6861507296562195\n",
      "loss:  0.7030210494995117\n",
      "loss:  0.6915483474731445\n",
      "loss:  0.7023593783378601\n",
      "loss:  0.6874260306358337\n",
      "loss:  0.6945696473121643\n",
      "loss:  0.6931275129318237\n",
      "loss:  0.6922361850738525\n",
      "loss:  0.6895502805709839\n",
      "loss:  0.6920630931854248\n",
      "loss:  0.6892428994178772\n",
      "loss:  0.6896821856498718\n",
      "loss:  0.6871032118797302\n",
      "loss:  0.6973738670349121\n",
      "loss:  0.6996668577194214\n",
      "loss:  0.7035900950431824\n",
      "loss:  0.7012751698493958\n",
      "loss:  0.6899724006652832\n",
      "loss:  0.7013344168663025\n",
      "loss:  0.6988739967346191\n",
      "loss:  0.6969453692436218\n",
      "loss:  0.6923598647117615\n",
      "loss:  0.6912811994552612\n",
      "loss:  0.7034268379211426\n",
      "loss:  0.6939378976821899\n",
      "loss:  0.6948530077934265\n",
      "loss:  0.6992554664611816\n",
      "loss:  0.690153181552887\n",
      "loss:  0.6940610408782959\n",
      "loss:  0.693195104598999\n",
      "loss:  0.6915128827095032\n",
      "loss:  0.6989029049873352\n",
      "loss:  0.6932617425918579\n",
      "loss:  0.6942859888076782\n",
      "loss:  0.7001835703849792\n",
      "loss:  0.705837070941925\n",
      "loss:  0.6917030811309814\n",
      "loss:  0.6948530673980713\n",
      "loss:  0.6928393244743347\n",
      "loss:  0.6907811164855957\n",
      "loss:  0.6882418394088745\n",
      "loss:  0.6929486393928528\n",
      "loss:  0.7051336765289307\n",
      "loss:  0.6930124163627625\n",
      "loss:  0.7000945210456848\n",
      "loss:  0.6910759210586548\n",
      "loss:  0.6959317326545715\n",
      "loss:  0.6946128606796265\n",
      "loss:  0.6908482313156128\n",
      "loss:  0.6872202157974243\n",
      "loss:  0.6921698451042175\n",
      "loss:  0.6889055371284485\n",
      "loss:  0.7013040781021118\n",
      "loss:  0.6923567056655884\n",
      "loss:  0.7036560773849487\n",
      "loss:  0.7004333734512329\n",
      "loss:  0.6930863857269287\n",
      "loss:  0.688318133354187\n",
      "loss:  0.6940645575523376\n",
      "loss:  0.6965614557266235\n",
      "loss:  0.6989783644676208\n",
      "loss:  0.6896470189094543\n",
      "loss:  0.7010778784751892\n",
      "loss:  0.6971155405044556\n",
      "loss:  0.6911106705665588\n",
      "loss:  0.7025405764579773\n",
      "loss:  0.6889554262161255\n",
      "loss:  0.6913877725601196\n",
      "loss:  0.6940918564796448\n",
      "loss:  0.6880459189414978\n",
      "loss:  0.6946868300437927\n",
      "loss:  0.6905447244644165\n",
      "loss:  0.6938735246658325\n",
      "loss:  0.689364492893219\n",
      "loss:  0.6873148679733276\n",
      "loss:  0.6983835697174072\n",
      "loss:  0.6982317566871643\n",
      "loss:  0.6962396502494812\n",
      "loss:  0.6966673731803894\n",
      "loss:  0.6957986950874329\n",
      "loss:  0.6942201256752014\n",
      "loss:  0.6956101059913635\n",
      "loss:  0.696003794670105\n",
      "loss:  0.6916705965995789\n",
      "loss:  0.6873281002044678\n",
      "loss:  0.6906034350395203\n",
      "loss:  0.6972417831420898\n",
      "loss:  0.6852912902832031\n",
      "loss:  0.690244197845459\n",
      "loss:  0.6908783912658691\n",
      "loss:  0.6945756077766418\n",
      "loss:  0.689247190952301\n",
      "loss:  0.6928805112838745\n",
      "loss:  0.6934905052185059\n",
      "loss:  0.6957893371582031\n",
      "loss:  0.6928689479827881\n",
      "loss:  0.6975211501121521\n",
      "loss:  0.6864800453186035\n",
      "loss:  0.6942047476768494\n",
      "loss:  0.6952575445175171\n",
      "loss:  0.694490373134613\n",
      "loss:  0.6968160271644592\n",
      "loss:  0.696058452129364\n",
      "loss:  0.6975624561309814\n",
      "loss:  0.6969027519226074\n",
      "loss:  0.6920354962348938\n",
      "loss:  0.6952785849571228\n",
      "loss:  0.6912040710449219\n",
      "loss:  0.6952061653137207\n",
      "loss:  0.6886875629425049\n",
      "loss:  0.6890203952789307\n",
      "loss:  0.6896759867668152\n",
      "loss:  0.6938077807426453\n",
      "loss:  0.689791738986969\n",
      "loss:  0.6936075687408447\n",
      "loss:  0.6966848969459534\n",
      "loss:  0.687961220741272\n",
      "loss:  0.6909153461456299\n",
      "loss:  0.697431743144989\n",
      "loss:  0.6870852708816528\n",
      "loss:  0.6890162229537964\n",
      "loss:  0.7013870477676392\n",
      "loss:  0.6953730583190918\n",
      "loss:  0.6990476846694946\n",
      "loss:  0.6913377046585083\n",
      "loss:  0.6910070180892944\n",
      "loss:  0.6933355927467346\n",
      "loss:  0.6945423483848572\n",
      "loss:  0.6939089894294739\n",
      "loss:  0.6926364302635193\n",
      "loss:  0.6887571811676025\n",
      "loss:  0.6914727091789246\n",
      "loss:  0.6956661939620972\n",
      "loss:  0.6944512724876404\n",
      "loss:  0.6954281330108643\n",
      "loss:  0.6959153413772583\n",
      "loss:  0.6839644312858582\n",
      "loss:  0.6956881880760193\n",
      "loss:  0.6953783631324768\n",
      "loss:  0.7009090185165405\n",
      "loss:  0.6908583045005798\n",
      "loss:  0.6967253088951111\n",
      "loss:  0.698390781879425\n",
      "loss:  0.6882990002632141\n",
      "loss:  0.7003889679908752\n",
      "loss:  0.6958402395248413\n",
      "loss:  0.6957635879516602\n",
      "loss:  0.6936416029930115\n",
      "loss:  0.6991961002349854\n",
      "loss:  0.6967458724975586\n",
      "loss:  0.6943875551223755\n",
      "loss:  0.685932993888855\n",
      "loss:  0.6937822103500366\n",
      "loss:  0.6984154582023621\n",
      "loss:  0.6923426389694214\n",
      "loss:  0.69449383020401\n",
      "loss:  0.6890364289283752\n",
      "loss:  0.6874967217445374\n",
      "loss:  0.7015361189842224\n",
      "loss:  0.6881566643714905\n",
      "loss:  0.6886464953422546\n",
      "loss:  0.6973394751548767\n",
      "loss:  0.6859112977981567\n",
      "loss:  0.6928645372390747\n",
      "loss:  0.6900845170021057\n",
      "loss:  0.6878795027732849\n",
      "loss:  0.6901260614395142\n",
      "loss:  0.6888241767883301\n",
      "loss:  0.6954659819602966\n",
      "loss:  0.6903184652328491\n",
      "loss:  0.6903510093688965\n",
      "loss:  0.6847859621047974\n",
      "loss:  0.6970987319946289\n",
      "loss:  0.6925097703933716\n",
      "loss:  0.6957263946533203\n",
      "loss:  0.6932823657989502\n",
      "loss:  0.6863241195678711\n",
      "loss:  0.6953977346420288\n",
      "loss:  0.6905210614204407\n",
      "loss:  0.6902796030044556\n",
      "loss:  0.6905078887939453\n",
      "loss:  0.6954911947250366\n",
      "loss:  0.6934241652488708\n",
      "loss:  0.6926124095916748\n",
      "loss:  0.6896571516990662\n",
      "loss:  0.6890109181404114\n",
      "loss:  0.692107081413269\n",
      "loss:  0.6925647258758545\n",
      "loss:  0.6979876756668091\n",
      "loss:  0.6930527687072754\n",
      "loss:  0.6865260004997253\n",
      "loss:  0.6903959512710571\n",
      "loss:  0.698901891708374\n",
      "loss:  0.6908548474311829\n",
      "loss:  0.687786877155304\n",
      "loss:  0.6924859285354614\n",
      "loss:  0.6900668144226074\n",
      "loss:  0.6979347467422485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6892625093460083\n",
      "loss:  0.6851351857185364\n",
      "loss:  0.6881669163703918\n",
      "loss:  0.6966465711593628\n",
      "loss:  0.6993347406387329\n",
      "loss:  0.6939584612846375\n",
      "loss:  0.6954267024993896\n",
      "loss:  0.6958765983581543\n",
      "loss:  0.6840575337409973\n",
      "loss:  0.6946519613265991\n",
      "loss:  0.6876624822616577\n",
      "loss:  0.6891136169433594\n",
      "loss:  0.6930837631225586\n",
      "loss:  0.6954424977302551\n",
      "loss:  0.701802134513855\n",
      "loss:  0.6865566372871399\n",
      "loss:  0.6937385201454163\n",
      "loss:  0.6980472803115845\n",
      "loss:  0.697300910949707\n",
      "loss:  0.6935340166091919\n",
      "loss:  0.6957088708877563\n",
      "loss:  0.6875801086425781\n",
      "loss:  0.7050045132637024\n",
      "loss:  0.7007207870483398\n",
      "loss:  0.683353841304779\n",
      "loss:  0.6876616477966309\n",
      "loss:  0.6881241202354431\n",
      "loss:  0.6931053400039673\n",
      "loss:  0.6848940849304199\n",
      "loss:  0.6881552934646606\n",
      "loss:  0.698803186416626\n",
      "loss:  0.6976219415664673\n",
      "loss:  0.6918325424194336\n",
      "loss:  0.6857380867004395\n",
      "loss:  0.7007657885551453\n",
      "loss:  0.6949061751365662\n",
      "loss:  0.6991812586784363\n",
      "loss:  0.6898170709609985\n",
      "loss:  0.6954881548881531\n",
      "loss:  0.7001193165779114\n",
      "loss:  0.6836449503898621\n",
      "loss:  0.6878679394721985\n",
      "loss:  0.6925318241119385\n",
      "loss:  0.6924070715904236\n",
      "loss:  0.6923714876174927\n",
      "loss:  0.6981281638145447\n",
      "loss:  0.6864851713180542\n",
      "loss:  0.6880448460578918\n",
      "loss:  0.6917142868041992\n",
      "loss:  0.7001191973686218\n",
      "loss:  0.7002040147781372\n",
      "loss:  0.6953409314155579\n",
      "loss:  0.6813139915466309\n",
      "loss:  0.6843686103820801\n",
      "loss:  0.6906750798225403\n",
      "loss:  0.6970652937889099\n",
      "loss:  0.701959490776062\n",
      "loss:  0.7018582820892334\n",
      "loss:  0.6982476115226746\n",
      "loss:  0.6919726133346558\n",
      "loss:  0.6931720972061157\n",
      "loss:  0.6987922787666321\n",
      "loss:  0.6987344026565552\n",
      "loss:  0.6921555995941162\n",
      "loss:  0.6831072568893433\n",
      "loss:  0.6991638541221619\n",
      "loss:  0.689773440361023\n",
      "loss:  0.6953721046447754\n",
      "loss:  0.6865701079368591\n",
      "loss:  0.6941157579421997\n",
      "loss:  0.6917951107025146\n",
      "loss:  0.692050576210022\n",
      "loss:  0.6942924857139587\n",
      "loss:  0.6915872097015381\n",
      "loss:  0.6946048736572266\n",
      "loss:  0.687122106552124\n",
      "loss:  0.6999252438545227\n",
      "loss:  0.6919316649436951\n",
      "loss:  0.6964529752731323\n",
      "loss:  0.6923754215240479\n",
      "loss:  0.6952399611473083\n",
      "loss:  0.6959323883056641\n",
      "loss:  0.6927830576896667\n",
      "loss:  0.6890920400619507\n",
      "loss:  0.7016085386276245\n",
      "loss:  0.6971327662467957\n",
      "loss:  0.6877990961074829\n",
      "loss:  0.6946868300437927\n",
      "loss:  0.6869281530380249\n",
      "loss:  0.6964007019996643\n",
      "loss:  0.6824672818183899\n",
      "loss:  0.6964343190193176\n",
      "loss:  0.684151291847229\n",
      "loss:  0.6980235576629639\n",
      "loss:  0.6933816075325012\n",
      "loss:  0.6964372992515564\n",
      "loss:  0.6987501382827759\n",
      "loss:  0.698578417301178\n",
      "loss:  0.6936642527580261\n",
      "loss:  0.6957953572273254\n",
      "loss:  0.6915041208267212\n",
      "loss:  0.6917178630828857\n",
      "loss:  0.7005995512008667\n",
      "loss:  0.6970654726028442\n",
      "loss:  0.694980263710022\n",
      "loss:  0.7012785077095032\n",
      "loss:  0.6857835054397583\n",
      "loss:  0.6953803300857544\n",
      "loss:  0.6922836303710938\n",
      "loss:  0.6950743198394775\n",
      "loss:  0.694878876209259\n",
      "loss:  0.6973473429679871\n",
      "loss:  0.6902071833610535\n",
      "loss:  0.6965193748474121\n",
      "loss:  0.6916592717170715\n",
      "loss:  0.6876838207244873\n",
      "loss:  0.6913186311721802\n",
      "loss:  0.7043758034706116\n",
      "loss:  0.6942654848098755\n",
      "loss:  0.6880570650100708\n",
      "loss:  0.6977365016937256\n",
      "loss:  0.6947209239006042\n",
      "loss:  0.7015466690063477\n",
      "loss:  0.7023199796676636\n",
      "loss:  0.6817257404327393\n",
      "loss:  0.6948975920677185\n",
      "loss:  0.6917232275009155\n",
      "loss:  0.6919302344322205\n",
      "loss:  0.689922034740448\n",
      "loss:  0.7041537165641785\n",
      "loss:  0.6896262168884277\n",
      "loss:  0.6975420713424683\n",
      "loss:  0.6938199996948242\n",
      "loss:  0.6969051361083984\n",
      "loss:  0.6902845501899719\n",
      "loss:  0.6935070753097534\n",
      "loss:  0.6922727823257446\n",
      "loss:  0.6904674172401428\n",
      "loss:  0.6940809488296509\n",
      "loss:  0.6919354200363159\n",
      "loss:  0.6964132785797119\n",
      "loss:  0.6917521953582764\n",
      "loss:  0.6930904388427734\n",
      "loss:  0.6974296569824219\n",
      "loss:  0.7025150060653687\n",
      "loss:  0.6907232999801636\n",
      "loss:  0.6991782188415527\n",
      "loss:  0.6962990760803223\n",
      "loss:  0.690322756767273\n",
      "loss:  0.6884605288505554\n",
      "loss:  0.6905221939086914\n",
      "loss:  0.7007541060447693\n",
      "loss:  0.694550633430481\n",
      "loss:  0.6949043273925781\n",
      "loss:  0.6906824707984924\n",
      "loss:  0.6948721408843994\n",
      "loss:  0.6897199153900146\n",
      "loss:  0.6997650265693665\n",
      "loss:  0.7020069360733032\n",
      "loss:  0.6960685849189758\n",
      "loss:  0.6981339454650879\n",
      "loss:  0.6902000904083252\n",
      "loss:  0.691805899143219\n",
      "loss:  0.6920026540756226\n",
      "loss:  0.6902400255203247\n",
      "loss:  0.6944226026535034\n",
      "loss:  0.6932418346405029\n",
      "loss:  0.695130467414856\n",
      "loss:  0.6918176412582397\n",
      "loss:  0.6930266618728638\n",
      "loss:  0.7033703923225403\n",
      "loss:  0.7025636434555054\n",
      "loss:  0.6979807019233704\n",
      "loss:  0.6981978416442871\n",
      "loss:  0.6932958364486694\n",
      "loss:  0.694258987903595\n",
      "loss:  0.6884344220161438\n",
      "loss:  0.6936966180801392\n",
      "loss:  0.6958296895027161\n",
      "loss:  0.6942328810691833\n",
      "loss:  0.6963852643966675\n",
      "loss:  0.6954683065414429\n",
      "loss:  0.6867170929908752\n",
      "loss:  0.6952481269836426\n",
      "loss:  0.6975215673446655\n",
      "loss:  0.6942492127418518\n",
      "loss:  0.6937127113342285\n",
      "loss:  0.6902363300323486\n",
      "loss:  0.6929876208305359\n",
      "loss:  0.6928335428237915\n",
      "loss:  0.694683313369751\n",
      "loss:  0.6920392513275146\n",
      "loss:  0.6910053491592407\n",
      "loss:  0.6903117895126343\n",
      "loss:  0.6870944499969482\n",
      "loss:  0.6928869485855103\n",
      "loss:  0.6873060464859009\n",
      "loss:  0.693356454372406\n",
      "loss:  0.701411783695221\n",
      "loss:  0.6949927806854248\n",
      "loss:  0.6948370933532715\n",
      "loss:  0.6915052533149719\n",
      "loss:  0.6960734128952026\n",
      "loss:  0.6969401240348816\n",
      "loss:  0.6987531185150146\n",
      "loss:  0.688052773475647\n",
      "loss:  0.6951471567153931\n",
      "loss:  0.6956963539123535\n",
      "loss:  0.694340705871582\n",
      "loss:  0.6899926662445068\n",
      "loss:  0.6887655258178711\n",
      "loss:  0.6878702044487\n",
      "loss:  0.6939064264297485\n",
      "loss:  0.6943474411964417\n",
      "loss:  0.6942290663719177\n",
      "loss:  0.6944301724433899\n",
      "loss:  0.6887732148170471\n",
      "loss:  0.6919546127319336\n",
      "loss:  0.6889438033103943\n",
      "loss:  0.6899957060813904\n",
      "loss:  0.6896222829818726\n",
      "loss:  0.697838544845581\n",
      "loss:  0.6908966898918152\n",
      "loss:  0.690747082233429\n",
      "loss:  0.6956881880760193\n",
      "loss:  0.6906780004501343\n",
      "loss:  0.6928318738937378\n",
      "loss:  0.6950719952583313\n",
      "loss:  0.6925562620162964\n",
      "loss:  0.6931608319282532\n",
      "loss:  0.6909363269805908\n",
      "loss:  0.6935293078422546\n",
      "loss:  0.6906880736351013\n",
      "loss:  0.6947845220565796\n",
      "loss:  0.69419926404953\n",
      "loss:  0.69685298204422\n",
      "loss:  0.6930164098739624\n",
      "loss:  0.6922962665557861\n",
      "loss:  0.6933377981185913\n",
      "loss:  0.6977143287658691\n",
      "loss:  0.6997631788253784\n",
      "loss:  0.6912978291511536\n",
      "loss:  0.6904013752937317\n",
      "loss:  0.6923328042030334\n",
      "loss:  0.6932961344718933\n",
      "loss:  0.6934889554977417\n",
      "loss:  0.6972326040267944\n",
      "loss:  0.689828097820282\n",
      "loss:  0.6957710981369019\n",
      "loss:  0.6893619894981384\n",
      "loss:  0.692119300365448\n",
      "loss:  0.6962974667549133\n",
      "loss:  0.702639639377594\n",
      "loss:  0.6956382989883423\n",
      "loss:  0.6914777159690857\n",
      "loss:  0.7008139491081238\n",
      "loss:  0.6936968564987183\n",
      "loss:  0.6937860250473022\n",
      "loss:  0.6917553544044495\n",
      "loss:  0.691706120967865\n",
      "loss:  0.6926977038383484\n",
      "loss:  0.6913809776306152\n",
      "loss:  0.6896472573280334\n",
      "loss:  0.6979182362556458\n",
      "loss:  0.6957451701164246\n",
      "loss:  0.696958065032959\n",
      "loss:  0.6895930767059326\n",
      "loss:  0.6947715282440186\n",
      "loss:  0.6930432319641113\n",
      "loss:  0.6944092512130737\n",
      "loss:  0.6977712512016296\n",
      "loss:  0.695929229259491\n",
      "loss:  0.6904598474502563\n",
      "loss:  0.6902914047241211\n",
      "loss:  0.6927348375320435\n",
      "loss:  0.6991561651229858\n",
      "loss:  0.699521005153656\n",
      "loss:  0.6926111578941345\n",
      "loss:  0.6998245120048523\n",
      "loss:  0.6967993378639221\n",
      "loss:  0.6883094310760498\n",
      "loss:  0.7005100250244141\n",
      "loss:  0.6911197304725647\n",
      "loss:  0.6895626783370972\n",
      "loss:  0.6892043352127075\n",
      "loss:  0.6854143142700195\n",
      "loss:  0.691718339920044\n",
      "loss:  0.691512405872345\n",
      "loss:  0.691996157169342\n",
      "loss:  0.6904961466789246\n",
      "loss:  0.6935485601425171\n",
      "loss:  0.6947702765464783\n",
      "loss:  0.6901138424873352\n",
      "loss:  0.6895357966423035\n",
      "loss:  0.6956110000610352\n",
      "loss:  0.6912809014320374\n",
      "loss:  0.6956175565719604\n",
      "loss:  0.687328577041626\n",
      "loss:  0.6953135132789612\n",
      "loss:  0.6826772689819336\n",
      "loss:  0.6957255005836487\n",
      "loss:  0.6914109587669373\n",
      "loss:  0.6872286796569824\n",
      "loss:  0.6893274784088135\n",
      "loss:  0.6893724799156189\n",
      "loss:  0.6945199966430664\n",
      "loss:  0.6984391212463379\n",
      "loss:  0.6983364224433899\n",
      "loss:  0.6982083916664124\n",
      "loss:  0.6879994869232178\n",
      "loss:  0.699578046798706\n",
      "loss:  0.7025444507598877\n",
      "loss:  0.6943528056144714\n",
      "loss:  0.6916887760162354\n",
      "loss:  0.7027398347854614\n",
      "loss:  0.6886798739433289\n",
      "loss:  0.6943619251251221\n",
      "loss:  0.695893406867981\n",
      "loss:  0.6944295167922974\n",
      "loss:  0.6970073580741882\n",
      "loss:  0.6804110407829285\n",
      "loss:  0.7007542252540588\n",
      "loss:  0.6958914399147034\n",
      "loss:  0.6822535395622253\n",
      "loss:  0.6856803297996521\n",
      "loss:  0.6871264576911926\n",
      "loss:  0.6968931555747986\n",
      "loss:  0.6925137042999268\n",
      "loss:  0.6945093274116516\n",
      "loss:  0.6918803453445435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6900596618652344\n",
      "loss:  0.6945833563804626\n",
      "loss:  0.6877343058586121\n",
      "loss:  0.6953093409538269\n",
      "loss:  0.6940040588378906\n",
      "loss:  0.6944358944892883\n",
      "loss:  0.6971844434738159\n",
      "loss:  0.7014098167419434\n",
      "loss:  0.6955775022506714\n",
      "loss:  0.6917064785957336\n",
      "loss:  0.6893587708473206\n",
      "loss:  0.6929960250854492\n",
      "loss:  0.699982225894928\n",
      "loss:  0.6965975761413574\n",
      "loss:  0.6893919110298157\n",
      "loss:  0.6968192458152771\n",
      "loss:  0.6871902346611023\n",
      "loss:  0.6861231923103333\n",
      "loss:  0.6928264498710632\n",
      "loss:  0.692450225353241\n",
      "loss:  0.6899158954620361\n",
      "loss:  0.7022206783294678\n",
      "loss:  0.6965035200119019\n",
      "loss:  0.6907980442047119\n",
      "loss:  0.6901686191558838\n",
      "loss:  0.6977433562278748\n",
      "loss:  0.6864941120147705\n",
      "loss:  0.6961396932601929\n",
      "loss:  0.6993504166603088\n",
      "loss:  0.6954563856124878\n",
      "loss:  0.687395453453064\n",
      "loss:  0.6868399381637573\n",
      "loss:  0.6931779980659485\n",
      "loss:  0.6855236291885376\n",
      "loss:  0.6852506995201111\n",
      "loss:  0.69000244140625\n",
      "loss:  0.6972134113311768\n",
      "loss:  0.6904947757720947\n",
      "loss:  0.692408561706543\n",
      "loss:  0.6898033618927002\n",
      "loss:  0.6911770105361938\n",
      "loss:  0.6969692707061768\n",
      "loss:  0.697171151638031\n",
      "loss:  0.6932080984115601\n",
      "loss:  0.688096821308136\n",
      "loss:  0.6933256983757019\n",
      "loss:  0.6926881074905396\n",
      "loss:  0.6940866112709045\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d089bcf17564>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "memory = model.initial_memory.to(device)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    optimiser.zero_grad()\n",
    "    output, output_memory = model(inputs, memory)\n",
    "    \n",
    "    loss = criterion(output.squeeze(1), labels.squeeze(1))\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"loss: \", loss.cpu().item())\n",
    "    \n",
    "    memory = output_memory.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
