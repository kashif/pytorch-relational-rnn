{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational RNNs by Adam Santoro et al. in PyTorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalMemory(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_size: \n",
    "        mem_slots: The total number of memory slots to use.\n",
    "        head_size: The size of an attention head.\n",
    "        num_heads: The number of attention heads to use. Defaults to 1.\n",
    "        num_blocks: Number of times to compute attention per time step. Defaults to 1.\n",
    "        forget_bias:\n",
    "        input_bias:\n",
    "        gate_style:\n",
    "        attention_mlp_layers:\n",
    "        key_size:\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size,\n",
    "                 mem_slots, head_size, num_heads=1, num_blocks=1,\n",
    "                 forget_bias=1.0, input_bias=0.0, gate_style='unit',\n",
    "                 attention_mlp_layers=2, key_size=None):\n",
    "        super(RelationalMemory, self).__init__()\n",
    "        \n",
    "        self._mem_slots = mem_slots\n",
    "        self._head_size = head_size\n",
    "        self._num_heads = num_heads\n",
    "        self._mem_size = self._head_size * self._num_heads\n",
    "\n",
    "        if num_blocks < 1:\n",
    "            raise ValueError('num_blocks must be >= 1. Got: {}.'.format(num_blocks))\n",
    "        self._num_blocks = num_blocks\n",
    "\n",
    "        self._forget_bias = forget_bias\n",
    "        self._input_bias = input_bias\n",
    "\n",
    "        if gate_style not in ['unit', 'memory', None]:\n",
    "            raise ValueError(\n",
    "                'gate_style must be one of [\\'unit\\', \\'memory\\', None]. Got: '\n",
    "                '{}.'.format(gate_style))\n",
    "        self._gate_style = gate_style\n",
    "\n",
    "        if attention_mlp_layers < 1:\n",
    "            raise ValueError('attention_mlp_layers must be >= 1. Got: {}.'.format(\n",
    "                attention_mlp_layers))\n",
    "        self._attention_mlp_layers = attention_mlp_layers\n",
    "\n",
    "        self._key_size = key_size if key_size else self._head_size\n",
    "\n",
    "        self._linear = nn.Linear(in_features=input_size,\n",
    "                                 out_features=self._mem_size)\n",
    "        \n",
    "        qkv_size = 2 * self._key_size + self._head_size\n",
    "        total_size = qkv_size * self._num_heads\n",
    "        self._attention_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                           out_features=total_size)\n",
    "        self._attention_layer_norm = nn.LayerNorm(total_size)\n",
    "        \n",
    "        attention_mlp_module = nn.ModuleList([nn.Sequential(\n",
    "                nn.Linear(in_features=self._mem_size,\n",
    "                          out_features=self._mem_size),\n",
    "                nn.ReLU())] * (self._attention_mlp_layers - 1) +\n",
    "                [nn.Linear(in_features=self._mem_size,\n",
    "                           out_features=self._mem_size)]\n",
    "        )\n",
    "        self._attention_mlp = nn.Sequential(*attention_mlp_module)\n",
    "        \n",
    "        self._attend_layer_norm_1 = nn.LayerNorm(self._mem_size)\n",
    "        self._attend_layer_norm_2 = nn.LayerNorm(self._mem_size)\n",
    "        \n",
    "        num_gates = 2 * self._calculate_gate_size()\n",
    "        self._gate_inputs_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                             out_features=num_gates)\n",
    "        \n",
    "        self._gate_memory_linear = nn.Linear(in_features=self._mem_size,\n",
    "                                             out_features=num_gates)\n",
    "        \n",
    "        \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\"Creates the initial memory.\n",
    "\n",
    "        We should ensure each row of the memory is initialized to be unique,\n",
    "        so initialize the matrix to be the identity. We then pad or truncate\n",
    "        as necessary so that init_state is of size (batch_size, self._mem_slots, self._mem_size).\n",
    "\n",
    "        Returns:\n",
    "            init_state: A truncated or padded matrix of size\n",
    "            (batch_size, self._mem_slots, self._mem_size).\n",
    "        \"\"\"    \n",
    "        init_state = torch.eye(n=self._mem_slots).repeat(batch_size, 1, 1)\n",
    "        \n",
    "        # Pad the matrix with zeros.\n",
    "        if self._mem_size > self._mem_slots:\n",
    "            difference = self._mem_size - self._mem_slots\n",
    "            pad = torch.zeros((batch_size, self._mem_slots, difference))\n",
    "            init_state = torch.cat([init_state, pad], dim=-1)\n",
    "        # Truncation. Take the first `self._mem_size` components.\n",
    "        elif self._mem_size < self._mem_slots:\n",
    "            init_state = init_state[:, :, :self._mem_size]\n",
    "        \n",
    "        return init_state\n",
    "        \n",
    "\n",
    "    def _multihead_attention(self, memory): # memory: [B, MEM_SLOT, MEM_SIZE]\n",
    "        # F = total_size\n",
    "        # mem_slots = MEM_SLOT = N\n",
    "        mem_slots = memory.size(1)\n",
    "        \n",
    "        # [B, MEM_SLOT, MEM_SIZE] -> [B*MEM_SLOT, MEM_SIZE] -> Linear -> [B*MEM_SLOT, F]\n",
    "        qkv = self._attention_linear(memory.view(-1, memory.size(2)))\n",
    "        \n",
    "        # [B*MEM_SLOT, F] -> Layer Norm -> [B*MEM_SLOT, F] -> [B, MEM_SLOT, F]\n",
    "        qkv = self._attention_layer_norm(qkv).view(memory.size(0), mem_slots, -1)\n",
    "        \n",
    "        # H = num_heads\n",
    "        qkv_size = 2 * self._key_size + self._head_size\n",
    "        \n",
    "        # [B, N, F] -> [B, N, H, F/H]\n",
    "        qkv_reshape = qkv.view(-1, mem_slots, self._num_heads, qkv_size)\n",
    "        \n",
    "        # [B, N, H, F/H] -> [B, H, N, F/H]\n",
    "        qkv_transpose = qkv_reshape.permute(0, 2, 1, 3)\n",
    "        # split q, k, v\n",
    "        q, k, v = torch.split(qkv_transpose, [self._key_size, self._key_size, self._head_size], dim=-1)\n",
    "        \n",
    "        q *= qkv_size ** -0.5\n",
    "        dot_product = torch.matmul(q, torch.transpose(k, 2, 3)) # [B, H, N, N]\n",
    "        weights = F.softmax(dot_product, dim=-1)\n",
    "        \n",
    "        #[B, H, N, V]\n",
    "        output = torch.matmul(weights, v)\n",
    "        \n",
    "        # [B, H, N, V] -> [B, N, H, V]\n",
    "        output_transpose = output.permute(0, 2, 1, 3)\n",
    "        \n",
    "        # [B, N, H, V] -> [B, N, H * V]\n",
    "        new_memory = output_transpose.contiguous().view(-1, output_transpose.size(1), \n",
    "                                                        output_transpose.size(2)*output_transpose.size(3))\n",
    "        \n",
    "        return new_memory #[B, MEM_SLOTS, MEM_SIZE]\n",
    "    \n",
    "    \n",
    "    def _attend_over_memory(self, memory):\n",
    "        # memory: [B, MEM_SLOT, MEM_SIZE]\n",
    "        for _ in range(self._num_blocks):\n",
    "            attended_memory = self._multihead_attention(memory) # [B, MEM_SLOT, MEM_SIZE]\n",
    "            \n",
    "            # add a skip connection the multiheaded attention's input.\n",
    "            # memory = LN_1(memory + attended_memory) [B*MEM_SLOT, MEM_SIZE]\n",
    "            memory = self._attend_layer_norm_1((memory + attended_memory).view(-1, memory.size(2)))\n",
    "            \n",
    "            # add a skip connection to the attention_mlp's input.\n",
    "            # memory = LN_2( MLP(memory) + memory)\n",
    "            memory = self._attend_layer_norm_2(self._attention_mlp(memory) + memory).view(-1, \n",
    "                                                                                          attended_memory.size(1),\n",
    "                                                                                          attended_memory.size(2))\n",
    "    \n",
    "        return memory\n",
    "    \n",
    "    def _calculate_gate_size(self):\n",
    "        if self._gate_style == 'unit':\n",
    "            return self._mem_size\n",
    "        elif self._gate_style == 'memory':\n",
    "          return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def _create_gates(self, inputs, memory):\n",
    "        memory = torch.tanh(memory)\n",
    "        \n",
    "        #inputs [B, 1, MEM_SIZE] -> [B, 1*MEM_SIZE]\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        \n",
    "        # [B, 1*MEM_SIZE] -> Linear -> [B, num_gates] -> [B, 1, num_gates]\n",
    "        gate_inputs = self._gate_inputs_linear(inputs).unsqueeze(1)\n",
    "        \n",
    "        # memory [B, MEM_SLOT, MEM_SIZE] -> [B*MEM_SLOT, MEM_SIZE] -> Linear -> [B*MEM_SLOT, 2*num_gates]\n",
    "        # -> [B, MEM_SLOT, 2*num_gates]\n",
    "        gate_memory = self._gate_memory_linear(memory.view(-1, memory.size(2))).view(memory.size(0),\n",
    "                                                                                     memory.size(1),\n",
    "                                                                                     -1)\n",
    "        \n",
    "        input_gate, forget_gate = torch.chunk(gate_memory + gate_inputs, 2, dim=2)\n",
    "        \n",
    "        input_gate = torch.sigmoid(input_gate + self._input_bias)\n",
    "        forget_gate = torch.sigmoid(forget_gate + self._forget_bias)\n",
    "        \n",
    "        return input_gate, forget_gate #[B, MEM_SLOT, num_gate], [B, MEM_SLOT, num_gate]\n",
    "        \n",
    "                                              \n",
    "    def forward(self, x, memory, treat_input_as_matrix=False):\n",
    "        # x: [B, T, F=input_size]\n",
    "        # memory: [B, MEM_SLOTS, MEM_SIZE]\n",
    "        batch_size = x.size(0)\n",
    "        total_timesteps = x.size(1)\n",
    "        \n",
    "        output_accumulator = x.new_zeros(batch_size, total_timesteps, self._mem_slots*self._mem_size)\n",
    "        \n",
    "        for index in range(total_timesteps):\n",
    "            # For each time-step\n",
    "            # inputs: [B, 1, F=input_size]\n",
    "            inputs = x[:,index].unsqueeze(1)\n",
    "            \n",
    "            if treat_input_as_matrix:\n",
    "                # [B, 1, F] -> [B*1, F] -> linear ->[B*1, MEM_SIZE] -> [B, 1, MEM_SIZE]\n",
    "                inputs_reshape =  self._linear(inputs.view(-1, inputs.size(2))).view(inputs.size(0), \n",
    "                                                                                     -1, \n",
    "                                                                                     self._mem_size)\n",
    "            else:\n",
    "                # [B, 1, F] -> [B, 1*F] -> linear -> [B, MEM_SIZE] -> [B, 1, MEM_SIZE]\n",
    "                inputs = inputs.view(inputs.size(0), -1)\n",
    "                inputs = self._linear(inputs)\n",
    "                inputs_reshape = inputs.unsqueeze(1)\n",
    "\n",
    "            # [B, MEM_SLOTS, MEM_SIZE] -> [B, MEM_SLOT+1, MEM_SIZE]\n",
    "            memory_plus_input = torch.cat([memory, inputs_reshape], dim=1)\n",
    "\n",
    "            next_memory = self._attend_over_memory(memory_plus_input)\n",
    "            n = inputs_reshape.size(1)\n",
    "            # [B, MEM_SLOT+1, MEM_SIZE] -> [B, MEM_SLOT, MEM_SIZE]\n",
    "            next_memory = next_memory[:, :-n, :]\n",
    "\n",
    "            if self._gate_style == 'unit' or self._gate_style == 'memory':\n",
    "                input_gate, forget_gate = self._create_gates(inputs_reshape, memory) #[B, MEM_SLOT, num_gate] \n",
    "                next_memory = input_gate * torch.tanh(next_memory)\n",
    "                next_memory += forget_gate * memory\n",
    "            \n",
    "            \n",
    "            # output: [B, MEM_SLOT, MEM_SIZE] -> [B, MEM_SLOT*MEM_SIZE]\n",
    "            output = next_memory.view(next_memory.size(0), -1)\n",
    "            \n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            \n",
    "            output_accumulator[:,index] = output.clone()\n",
    "            memory = next_memory.clone()\n",
    "        \n",
    "            \n",
    "        return output_accumulator, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_slots = 4\n",
    "head_size = 32\n",
    "num_heads = 2\n",
    "batch_size = 5\n",
    "input_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (batch_size, 3, 3)\n",
    "inputs = torch.Tensor(batch_size, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = RelationalMemory(input_size, mem_slots, head_size, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = mem.initial_state(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 64])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-154-df4d928e84b0>(231)forward()\n",
      "-> output_accumulator[:,index] = output.clone()\n",
      "(Pdb) output\n",
      "tensor([[ 8.1520e-39,  2.4029e-39, -3.1129e-39,  ...,  1.9256e-39,\n",
      "          6.0111e-01,  3.5943e-39],\n",
      "        [ 1.0000e+00,  2.3586e-01, -3.4278e-39,  ...,  4.2129e-01,\n",
      "          6.6003e-01,  8.1420e-01],\n",
      "        [ 1.2292e+00,  1.6309e-01, -2.9620e-01,  ...,  7.5471e-02,\n",
      "          4.4334e-01,  4.8905e-01],\n",
      "        [ 1.0000e+00,  2.3586e-01, -3.4278e-39,  ...,  4.2129e-01,\n",
      "          6.6003e-01,  8.1420e-01],\n",
      "        [ 1.0000e+00,  2.4029e-39, -3.1129e-39,  ...,  1.9256e-39,\n",
      "          2.4982e-39,  3.5943e-39]], grad_fn=<ViewBackward>)\n",
      "(Pdb) next_memory\n",
      "tensor([[[ 8.1520e-39,  2.4029e-39, -3.1129e-39,  ...,  1.9006e-39,\n",
      "           5.7813e-01,  3.5945e-39],\n",
      "         [ 1.3520e-39,  1.0000e+00, -3.1828e-39,  ...,  1.9116e-39,\n",
      "           5.4240e-01,  3.5344e-39],\n",
      "         [ 1.5484e-39,  2.8382e-39,  1.0000e+00,  ...,  2.6885e-39,\n",
      "           7.0168e-01,  3.7286e-39],\n",
      "         [ 5.3364e-40,  3.1439e-39, -3.2696e-39,  ...,  1.9256e-39,\n",
      "           6.0111e-01,  3.5943e-39]],\n",
      "\n",
      "        [[ 1.0000e+00,  2.3586e-01, -3.4278e-39,  ...,  4.0676e-01,\n",
      "           6.2567e-01,  8.3771e-01],\n",
      "         [ 2.8102e-39,  9.3654e-01, -3.4766e-39,  ...,  4.5900e-01,\n",
      "           6.3057e-01,  8.5825e-01],\n",
      "         [ 3.1612e-39,  5.2193e-01,  1.0000e+00,  ...,  5.1440e-01,\n",
      "           7.1665e-01,  8.5299e-01],\n",
      "         [ 2.1809e-39,  6.3495e-01, -3.2199e-39,  ...,  4.2129e-01,\n",
      "           6.6003e-01,  8.1420e-01]],\n",
      "\n",
      "        [[ 1.2292e+00,  1.6309e-01, -2.9620e-01,  ...,  1.1594e-01,\n",
      "           4.1216e-01,  5.0666e-01],\n",
      "         [ 2.4281e-01,  1.1486e+00, -3.1076e-01,  ...,  1.4462e-01,\n",
      "           4.0618e-01,  4.8764e-01],\n",
      "         [ 2.8804e-01,  2.9006e-01,  8.2712e-01,  ...,  1.4184e-01,\n",
      "           4.6856e-01,  5.0645e-01],\n",
      "         [ 1.6438e-01,  3.0949e-01, -3.1079e-01,  ...,  7.5471e-02,\n",
      "           4.4334e-01,  4.8905e-01]],\n",
      "\n",
      "        [[ 1.0000e+00,  2.3586e-01, -3.4278e-39,  ...,  4.0676e-01,\n",
      "           6.2567e-01,  8.3771e-01],\n",
      "         [ 2.8102e-39,  9.3654e-01, -3.4766e-39,  ...,  4.5900e-01,\n",
      "           6.3057e-01,  8.5825e-01],\n",
      "         [ 3.1612e-39,  5.2193e-01,  1.0000e+00,  ...,  5.1440e-01,\n",
      "           7.1665e-01,  8.5299e-01],\n",
      "         [ 2.1809e-39,  6.3495e-01, -3.2199e-39,  ...,  4.2129e-01,\n",
      "           6.6003e-01,  8.1420e-01]],\n",
      "\n",
      "        [[ 1.0000e+00,  2.4029e-39, -3.1129e-39,  ...,  1.9006e-39,\n",
      "           2.4027e-39,  3.5945e-39],\n",
      "         [ 1.3520e-39,  1.0000e+00, -3.1828e-39,  ...,  1.9116e-39,\n",
      "           2.2542e-39,  3.5344e-39],\n",
      "         [ 1.5484e-39,  2.8382e-39,  1.0000e+00,  ...,  2.6885e-39,\n",
      "           2.9162e-39,  3.7286e-39],\n",
      "         [ 5.3364e-40,  3.1439e-39, -3.2696e-39,  ...,  1.9256e-39,\n",
      "           2.4982e-39,  3.5943e-39]]], grad_fn=<ThAddBackward>)\n",
      "(Pdb) c\n",
      "> <ipython-input-154-df4d928e84b0>(229)forward()\n",
      "-> pdb.set_trace()\n",
      "(Pdb) output\n",
      "tensor([[-3.0871e-01,  2.3065e-01, -2.2557e-01,  ..., -1.1408e-01,\n",
      "          8.9160e-01,  2.1400e-01],\n",
      "        [ 1.0000e+00,  2.3586e-01, -4.4666e-39,  ...,  4.2129e-01,\n",
      "          6.6003e-01,  8.1420e-01],\n",
      "        [ 1.2292e+00, -2.4101e-01, -2.9620e-01,  ...,  1.2519e-01,\n",
      "          3.1801e-01,  1.2812e+00],\n",
      "        [ 1.0000e+00, -2.5570e-39, -4.4666e-39,  ...,  4.2129e-01,\n",
      "          6.6003e-01,  1.7064e+00],\n",
      "        [ 1.1106e+00,  2.7187e-01, -2.0986e-01,  ..., -4.7548e-02,\n",
      "         -2.2077e-01,  2.0690e-01]], grad_fn=<ViewBackward>)\n",
      "(Pdb) next_memory\n",
      "tensor([[[-3.0871e-01,  2.3065e-01, -2.2557e-01,  ..., -9.6309e-02,\n",
      "           8.7915e-01,  2.2451e-01],\n",
      "         [-2.9737e-01,  1.2194e+00, -2.2304e-01,  ..., -9.3509e-02,\n",
      "           8.1720e-01,  2.2110e-01],\n",
      "         [-3.1486e-01,  2.5259e-01,  7.7104e-01,  ..., -9.4667e-02,\n",
      "           1.0243e+00,  2.2159e-01],\n",
      "         [-3.0106e-01,  2.4869e-01, -2.3402e-01,  ..., -1.1408e-01,\n",
      "           8.9160e-01,  2.1400e-01]],\n",
      "\n",
      "        [[ 1.0000e+00,  2.3586e-01, -4.4666e-39,  ...,  4.0676e-01,\n",
      "           6.2567e-01,  8.3771e-01],\n",
      "         [ 4.1661e-39,  9.3654e-01, -4.3253e-39,  ...,  4.5900e-01,\n",
      "           6.3057e-01,  8.5825e-01],\n",
      "         [ 4.6246e-39,  5.2193e-01,  1.0000e+00,  ...,  5.1440e-01,\n",
      "           7.1665e-01,  8.5299e-01],\n",
      "         [ 3.4277e-39,  6.3495e-01, -4.1487e-39,  ...,  4.2129e-01,\n",
      "           6.6003e-01,  8.1420e-01]],\n",
      "\n",
      "        [[ 1.2292e+00, -2.4101e-01, -2.9620e-01,  ...,  1.6245e-01,\n",
      "           2.1024e-01,  1.2943e+00],\n",
      "         [ 2.4281e-01,  6.7926e-01, -3.1076e-01,  ...,  2.4347e-01,\n",
      "           1.7316e-01,  1.2902e+00],\n",
      "         [ 2.8804e-01, -7.7821e-02,  8.2712e-01,  ...,  2.8019e-01,\n",
      "           3.4148e-01,  1.3069e+00],\n",
      "         [ 1.6438e-01, -2.4508e-02, -3.1079e-01,  ...,  1.2519e-01,\n",
      "           3.1801e-01,  1.2812e+00]],\n",
      "\n",
      "        [[ 1.0000e+00, -2.5570e-39, -4.4666e-39,  ...,  4.0676e-01,\n",
      "           6.2567e-01,  1.7329e+00],\n",
      "         [ 4.1661e-39,  1.8087e-39, -4.3253e-39,  ...,  4.5900e-01,\n",
      "           6.3057e-01,  1.7571e+00],\n",
      "         [ 4.6246e-39, -8.9809e-40,  1.0000e+00,  ...,  5.1440e-01,\n",
      "           7.1665e-01,  1.7444e+00],\n",
      "         [ 3.4277e-39, -1.3714e-40, -4.1487e-39,  ...,  4.2129e-01,\n",
      "           6.6003e-01,  1.7064e+00]],\n",
      "\n",
      "        [[ 1.1106e+00,  2.7187e-01, -2.0986e-01,  ..., -5.3830e-02,\n",
      "          -2.4540e-01,  2.0910e-01],\n",
      "         [ 1.6456e-01,  1.2492e+00, -2.1627e-01,  ..., -4.3787e-02,\n",
      "          -2.4414e-01,  1.9695e-01],\n",
      "         [ 1.6889e-01,  2.8534e-01,  7.5924e-01,  ..., -6.2575e-02,\n",
      "          -2.0403e-01,  2.0416e-01],\n",
      "         [ 1.5674e-01,  2.6809e-01, -2.1947e-01,  ..., -4.7548e-02,\n",
      "          -2.2077e-01,  2.0690e-01]]], grad_fn=<ThAddBackward>)\n",
      "(Pdb) c\n",
      "> <ipython-input-154-df4d928e84b0>(231)forward()\n",
      "-> output_accumulator[:,index] = output.clone()\n",
      "(Pdb) c\n"
     ]
    }
   ],
   "source": [
    "output, next_memory = mem(inputs, init_state, treat_input_as_matrix=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 256])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 64])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_memory.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 64])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1466e+00, -4.0538e-02, -6.4860e-02,  ...,  3.7660e-01,\n",
       "           4.0484e-01, -3.2260e-01],\n",
       "         [ 1.3230e+00, -2.9083e-01, -4.1117e-01,  ...,  3.6210e-01,\n",
       "           5.4113e-01, -5.0793e-01],\n",
       "         [ 1.4667e+00, -3.7545e-01, -6.5924e-01,  ...,  1.9481e-01,\n",
       "           5.6986e-01, -7.0246e-01]],\n",
       "\n",
       "        [[ 9.6494e-01,  1.2526e-01,  3.2965e-01,  ...,  7.4273e-40,\n",
       "           3.8232e-39, -7.1376e-01],\n",
       "         [ 1.1437e+00, -1.1511e-01,  6.7471e-03,  ..., -4.6366e-01,\n",
       "          -2.0088e-01, -8.3691e-01],\n",
       "         [ 1.3293e+00, -3.5907e-01, -2.7712e-01,  ..., -9.8271e-01,\n",
       "          -3.6156e-01, -9.1689e-01]],\n",
       "\n",
       "        [[ 1.1466e+00, -4.0538e-02, -6.4860e-02,  ...,  3.7660e-01,\n",
       "           4.0484e-01, -3.2260e-01],\n",
       "         [ 1.3230e+00, -2.9083e-01, -4.1117e-01,  ...,  3.6210e-01,\n",
       "           5.4113e-01, -5.0793e-01],\n",
       "         [ 1.4667e+00, -3.7545e-01, -6.5924e-01,  ...,  1.9481e-01,\n",
       "           5.6986e-01, -7.0246e-01]],\n",
       "\n",
       "        [[ 1.1466e+00, -4.0538e-02, -6.4860e-02,  ...,  3.7660e-01,\n",
       "           4.0484e-01, -3.2260e-01],\n",
       "         [ 1.3230e+00, -2.9083e-01, -4.1117e-01,  ...,  3.6210e-01,\n",
       "           5.4113e-01, -5.0793e-01],\n",
       "         [ 1.4667e+00, -3.7544e-01, -6.5924e-01,  ...,  1.9481e-01,\n",
       "           5.6986e-01, -7.0246e-01]],\n",
       "\n",
       "        [[ 1.1466e+00, -4.0538e-02, -6.4860e-02,  ...,  3.7660e-01,\n",
       "           4.0484e-01, -3.2260e-01],\n",
       "         [ 1.3230e+00, -2.9083e-01, -4.1117e-01,  ...,  3.6210e-01,\n",
       "           5.4113e-01, -5.0793e-01],\n",
       "         [ 1.4667e+00, -3.7545e-01, -6.5924e-01,  ...,  1.9481e-01,\n",
       "           5.6986e-01, -7.0246e-01]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4667, -0.3754, -0.6592,  ...,  0.1249,  0.5104, -0.7486],\n",
       "         [ 1.0975,  0.7981, -0.7753,  ...,  0.2788,  0.5345, -0.7651],\n",
       "         [ 1.0546, -0.4408,  0.8206,  ...,  0.0380,  0.4739, -0.7498],\n",
       "         [ 1.0674, -0.3984, -0.7312,  ...,  0.1948,  0.5699, -0.7025]],\n",
       "\n",
       "        [[ 1.3293, -0.3591, -0.2771,  ..., -0.9504, -0.3630, -0.9444],\n",
       "         [ 1.1170,  0.5842, -0.5348,  ..., -0.9578, -0.4060, -0.9993],\n",
       "         [ 0.9862, -0.2343,  0.4565,  ..., -0.9901, -0.3731, -1.0200],\n",
       "         [ 0.9264, -0.2506, -0.4436,  ..., -0.9827, -0.3616, -0.9169]],\n",
       "\n",
       "        [[ 1.4667, -0.3754, -0.6592,  ...,  0.1249,  0.5104, -0.7486],\n",
       "         [ 1.0975,  0.7981, -0.7753,  ...,  0.2788,  0.5345, -0.7651],\n",
       "         [ 1.0546, -0.4408,  0.8206,  ...,  0.0380,  0.4739, -0.7498],\n",
       "         [ 1.0674, -0.3984, -0.7312,  ...,  0.1948,  0.5699, -0.7025]],\n",
       "\n",
       "        [[ 1.4667, -0.3754, -0.6592,  ...,  0.1249,  0.5104, -0.7486],\n",
       "         [ 1.0975,  0.7981, -0.7753,  ...,  0.2788,  0.5345, -0.7651],\n",
       "         [ 1.0546, -0.4408,  0.8206,  ...,  0.0380,  0.4739, -0.7498],\n",
       "         [ 1.0674, -0.3984, -0.7312,  ...,  0.1948,  0.5699, -0.7025]],\n",
       "\n",
       "        [[ 1.4667, -0.3754, -0.6592,  ...,  0.1249,  0.5104, -0.7486],\n",
       "         [ 1.0975,  0.7981, -0.7753,  ...,  0.2788,  0.5345, -0.7651],\n",
       "         [ 1.0546, -0.4408,  0.8206,  ...,  0.0380,  0.4739, -0.7498],\n",
       "         [ 1.0674, -0.3984, -0.7312,  ...,  0.1948,  0.5699, -0.7025]]],\n",
       "       grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
